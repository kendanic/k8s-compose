
==> Audit <==
|--------------|----------------------------|----------|------|---------|---------------------|---------------------|
|   Command    |            Args            | Profile  | User | Version |     Start Time      |      End Time       |
|--------------|----------------------------|----------|------|---------|---------------------|---------------------|
| start        | --driver=docker            | minikube | ken  | v1.35.0 | 19 Feb 25 11:10 IST |                     |
| delete       |                            | minikube | ken  | v1.35.0 | 19 Feb 25 11:21 IST | 19 Feb 25 11:21 IST |
| start        | --driver=docker            | minikube | ken  | v1.35.0 | 19 Feb 25 11:21 IST | 19 Feb 25 11:22 IST |
| start        |                            | minikube | ken  | v1.35.0 | 19 Feb 25 11:23 IST | 19 Feb 25 11:24 IST |
| delete       |                            | minikube | ken  | v1.35.0 | 15 Mar 25 19:01 IST | 15 Mar 25 19:01 IST |
| start        |                            | minikube | ken  | v1.35.0 | 15 Mar 25 19:01 IST | 15 Mar 25 19:02 IST |
| kubectl      | -- get po -A               | minikube | ken  | v1.35.0 | 15 Mar 25 19:03 IST | 15 Mar 25 19:04 IST |
| kubectl      | -- get po -A               | minikube | ken  | v1.35.0 | 15 Mar 25 22:15 IST | 15 Mar 25 22:15 IST |
| dashboard    |                            | minikube | ken  | v1.35.0 | 15 Mar 25 22:16 IST |                     |
| start        |                            | minikube | ken  | v1.35.0 | 20 Mar 25 13:33 IST | 20 Mar 25 13:34 IST |
| dashboard    |                            | minikube | ken  | v1.35.0 | 20 Mar 25 13:34 IST |                     |
| stop         |                            | minikube | ken  | v1.35.0 | 20 Mar 25 13:34 IST | 20 Mar 25 13:34 IST |
| start        |                            | minikube | ken  | v1.35.0 | 20 Mar 25 13:34 IST | 20 Mar 25 13:35 IST |
| dashboard    |                            | minikube | ken  | v1.35.0 | 20 Mar 25 13:35 IST |                     |
| dashboard    |                            | minikube | ken  | v1.35.0 | 20 Mar 25 13:35 IST |                     |
| service      | hello-node                 | minikube | ken  | v1.35.0 | 20 Mar 25 14:54 IST | 20 Mar 25 14:54 IST |
| service      | nginx-pod --url            | minikube | ken  | v1.35.0 | 20 Mar 25 15:20 IST |                     |
| service      | list                       | minikube | ken  | v1.35.0 | 20 Mar 25 15:20 IST | 20 Mar 25 15:20 IST |
| service      | nginx-pod --url            | minikube | ken  | v1.35.0 | 20 Mar 25 15:44 IST | 20 Mar 25 15:44 IST |
| service      | nginx-pod --url            | minikube | ken  | v1.35.0 | 20 Mar 25 15:50 IST | 20 Mar 25 15:50 IST |
| service      | nginx-pod --url            | minikube | ken  | v1.35.0 | 20 Mar 25 15:51 IST | 20 Mar 25 15:51 IST |
| delete       |                            | minikube | ken  | v1.35.0 | 20 Mar 25 15:53 IST | 20 Mar 25 15:53 IST |
| start        |                            | minikube | ken  | v1.35.0 | 20 Mar 25 15:53 IST | 20 Mar 25 15:54 IST |
| delete       |                            | minikube | ken  | v1.35.0 | 20 Mar 25 15:57 IST | 20 Mar 25 15:57 IST |
| start        |                            | minikube | ken  | v1.35.0 | 20 Mar 25 15:57 IST | 20 Mar 25 15:58 IST |
| dashboard    |                            | minikube | ken  | v1.35.0 | 20 Mar 25 17:39 IST |                     |
| start        |                            | minikube | ken  | v1.35.0 | 15 Apr 25 22:14 IST | 15 Apr 25 22:15 IST |
| service      | nginx --url                | minikube | ken  | v1.35.0 | 15 Apr 25 22:16 IST |                     |
| service      | nginx --url                | minikube | ken  | v1.35.0 | 15 Apr 25 22:16 IST | 15 Apr 25 22:16 IST |
| start        |                            | minikube | ken  | v1.35.0 | 28 Jun 25 12:45 IST | 28 Jun 25 12:46 IST |
| dashboard    |                            | minikube | ken  | v1.35.0 | 28 Jun 25 12:46 IST |                     |
| stop         |                            | minikube | ken  | v1.35.0 | 28 Jun 25 12:50 IST | 28 Jun 25 12:50 IST |
| start        |                            | minikube | ken  | v1.35.0 | 28 Jun 25 20:41 IST | 28 Jun 25 20:41 IST |
| stop         |                            | minikube | ken  | v1.35.0 | 28 Jun 25 21:45 IST | 28 Jun 25 21:45 IST |
| start        |                            | minikube | ken  | v1.35.0 | 24 Jul 25 20:33 IST |                     |
| addons       | enable dashboard           | minikube | ken  | v1.35.0 | 24 Jul 25 20:39 IST |                     |
| addons       | enable storage-provisioner | minikube | ken  | v1.35.0 | 24 Jul 25 20:41 IST |                     |
| delete       |                            | minikube | ken  | v1.36.0 | 24 Jul 25 20:44 IST | 24 Jul 25 20:44 IST |
| start        |                            | minikube | ken  | v1.36.0 | 24 Jul 25 20:44 IST | 24 Jul 25 20:48 IST |
| start        |                            | minikube | ken  | v1.36.0 | 24 Jul 25 20:51 IST |                     |
| start        |                            | minikube | ken  | v1.36.0 | 24 Jul 25 20:51 IST | 24 Jul 25 20:52 IST |
| start        |                            | minikube | ken  | v1.36.0 | 25 Jul 25 20:21 IST | 25 Jul 25 20:21 IST |
| dashboard    | --url                      | minikube | ken  | v1.36.0 | 25 Jul 25 20:21 IST |                     |
| service      | hello-node                 | minikube | ken  | v1.36.0 | 25 Jul 25 20:36 IST | 25 Jul 25 20:36 IST |
| service      | hello-node                 | minikube | ken  | v1.36.0 | 25 Jul 25 20:36 IST | 25 Jul 25 20:36 IST |
| service      | hello-node --url           | minikube | ken  | v1.36.0 | 25 Jul 25 20:38 IST | 25 Jul 25 20:38 IST |
| addons       | list                       | minikube | ken  | v1.36.0 | 25 Jul 25 20:41 IST | 25 Jul 25 20:41 IST |
| addons       | enable metrics-server      | minikube | ken  | v1.36.0 | 25 Jul 25 20:41 IST | 25 Jul 25 20:41 IST |
| addons       | list                       | minikube | ken  | v1.36.0 | 25 Jul 25 20:41 IST | 25 Jul 25 20:41 IST |
| service      | hello-node --url           | minikube | ken  | v1.36.0 | 25 Jul 25 20:44 IST | 25 Jul 25 20:44 IST |
| service      | nginx --url                | minikube | ken  | v1.36.0 | 25 Jul 25 20:44 IST |                     |
| service      | list                       | minikube | ken  | v1.36.0 | 25 Jul 25 20:44 IST | 25 Jul 25 20:44 IST |
| service      | nginx --url                | minikube | ken  | v1.36.0 | 25 Jul 25 20:46 IST | 25 Jul 25 20:46 IST |
| start        |                            | minikube | ken  | v1.36.0 | 26 Jul 25 10:48 IST | 26 Jul 25 10:49 IST |
| update-check |                            | minikube | ken  | v1.36.0 | 26 Jul 25 11:34 IST | 26 Jul 25 11:34 IST |
| service      | nginx-service --url        | minikube | ken  | v1.36.0 | 26 Jul 25 14:05 IST |                     |
| service      | nginx --url                | minikube | ken  | v1.36.0 | 26 Jul 25 14:05 IST |                     |
| service      | nginx-service --url        | minikube | ken  | v1.36.0 | 26 Jul 25 14:05 IST |                     |
| service      | nginx-pod --url            | minikube | ken  | v1.36.0 | 26 Jul 25 14:05 IST |                     |
| service      | nginx-service              | minikube | ken  | v1.36.0 | 26 Jul 25 14:07 IST |                     |
|--------------|----------------------------|----------|------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/07/26 10:48:56
Running on machine: RAD-IT-L13
Binary: Built with gc go1.24.0 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0726 10:48:56.364706    6652 out.go:345] Setting OutFile to fd 1 ...
I0726 10:48:56.366437    6652 out.go:397] isatty.IsTerminal(1) = true
I0726 10:48:56.366440    6652 out.go:358] Setting ErrFile to fd 2...
I0726 10:48:56.366446    6652 out.go:397] isatty.IsTerminal(2) = true
I0726 10:48:56.367401    6652 root.go:338] Updating PATH: /home/ken/.minikube/bin
I0726 10:48:56.368839    6652 out.go:352] Setting JSON to false
I0726 10:48:56.388480    6652 start.go:130] hostinfo: {"hostname":"RAD-IT-L13","uptime":757,"bootTime":1753506380,"procs":257,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"20.04","kernelVersion":"5.15.0-139-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"0f56e094-b048-4f72-b5cb-0205dee03c91"}
I0726 10:48:56.388553    6652 start.go:140] virtualization: kvm host
I0726 10:48:56.389951    6652 out.go:177] 😄  minikube v1.36.0 on Ubuntu 20.04
I0726 10:48:56.392205    6652 notify.go:220] Checking for updates...
I0726 10:48:56.393190    6652 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0726 10:48:56.394548    6652 driver.go:404] Setting default libvirt URI to qemu:///system
I0726 10:48:56.722047    6652 docker.go:123] docker version: linux-28.1.1:Docker Engine - Community
I0726 10:48:56.722263    6652 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0726 10:48:57.690659    6652 info.go:266] docker info: {ID:ecbcfa82-be56-42d3-ba7a-df31e6eb9530 Containers:2 ContainersRunning:0 ContainersPaused:0 ContainersStopped:2 Images:6 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:37 OomKillDisable:true NGoroutines:161 SystemTime:2025-07-26 10:48:57.655733528 +0530 IST LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.15.0-139-generic OperatingSystem:Ubuntu 20.04.6 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:8198180864 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:RAD-IT-L13 Labels:[] ExperimentalBuild:false ServerVersion:28.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID:ts5rg1wifof3izhmtu9tckxr4 NodeAddr:192.168.1.5 LocalNodeState:active ControlAvailable:true Error: RemoteManagers:[map[Addr:192.168.1.5:2377 NodeID:ts5rg1wifof3izhmtu9tckxr4]]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=builtin] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.23.0] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.35.1]] Warnings:<nil>}}
I0726 10:48:57.690804    6652 docker.go:318] overlay module found
I0726 10:48:57.691934    6652 out.go:177] ✨  Using the docker driver based on existing profile
I0726 10:48:57.693351    6652 start.go:304] selected driver: docker
I0726 10:48:57.693358    6652 start.go:908] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/ken:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0726 10:48:57.693439    6652 start.go:919] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0726 10:48:57.693548    6652 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0726 10:48:57.769064    6652 info.go:266] docker info: {ID:ecbcfa82-be56-42d3-ba7a-df31e6eb9530 Containers:2 ContainersRunning:0 ContainersPaused:0 ContainersStopped:2 Images:6 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:37 OomKillDisable:true NGoroutines:161 SystemTime:2025-07-26 10:48:57.758504387 +0530 IST LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.15.0-139-generic OperatingSystem:Ubuntu 20.04.6 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:8198180864 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:RAD-IT-L13 Labels:[] ExperimentalBuild:false ServerVersion:28.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID:ts5rg1wifof3izhmtu9tckxr4 NodeAddr:192.168.1.5 LocalNodeState:active ControlAvailable:true Error: RemoteManagers:[map[Addr:192.168.1.5:2377 NodeID:ts5rg1wifof3izhmtu9tckxr4]]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=builtin] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.23.0] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.35.1]] Warnings:<nil>}}
I0726 10:48:57.787750    6652 cni.go:84] Creating CNI manager for ""
I0726 10:48:57.788331    6652 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0726 10:48:57.788397    6652 start.go:347] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/ken:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0726 10:48:57.789393    6652 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0726 10:48:57.790155    6652 cache.go:121] Beginning downloading kic base image for docker with docker
I0726 10:48:57.790951    6652 out.go:177] 🚜  Pulling base image v0.0.47 ...
I0726 10:48:57.792755    6652 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local docker daemon
I0726 10:48:57.793055    6652 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0726 10:48:57.793110    6652 preload.go:146] Found local preload: /home/ken/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4
I0726 10:48:57.793116    6652 cache.go:56] Caching tarball of preloaded images
I0726 10:48:57.793232    6652 preload.go:172] Found /home/ken/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0726 10:48:57.793241    6652 cache.go:59] Finished verifying existence of preloaded tar for v1.33.1 on docker
I0726 10:48:57.793361    6652 profile.go:143] Saving config to /home/ken/.minikube/profiles/minikube/config.json ...
I0726 10:48:57.817159    6652 image.go:100] Found gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local docker daemon, skipping pull
I0726 10:48:57.817177    6652 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b exists in daemon, skipping load
I0726 10:48:57.817192    6652 cache.go:230] Successfully downloaded all kic artifacts
I0726 10:48:57.817236    6652 start.go:360] acquireMachinesLock for minikube: {Name:mk172bd60ef5bfff997fd3276e1d16fbc09f39e2 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0726 10:48:57.817343    6652 start.go:364] duration metric: took 81.846µs to acquireMachinesLock for "minikube"
I0726 10:48:57.817360    6652 start.go:96] Skipping create...Using existing machine configuration
I0726 10:48:57.817365    6652 fix.go:54] fixHost starting: 
I0726 10:48:57.817662    6652 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0726 10:48:57.850132    6652 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0726 10:48:57.850168    6652 fix.go:138] unexpected machine state, will restart: <nil>
I0726 10:48:57.851847    6652 out.go:177] 🔄  Restarting existing docker container for "minikube" ...
I0726 10:48:57.852843    6652 cli_runner.go:164] Run: docker start minikube
I0726 10:48:58.367337    6652 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0726 10:48:58.386216    6652 kic.go:430] container "minikube" state is running.
I0726 10:48:58.386518    6652 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0726 10:48:58.408519    6652 profile.go:143] Saving config to /home/ken/.minikube/profiles/minikube/config.json ...
I0726 10:48:58.408782    6652 machine.go:93] provisionDockerMachine start ...
I0726 10:48:58.408865    6652 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0726 10:48:58.431934    6652 main.go:141] libmachine: Using SSH client type: native
I0726 10:48:58.432543    6652 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0726 10:48:58.432550    6652 main.go:141] libmachine: About to run SSH command:
hostname
I0726 10:48:58.433221    6652 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:42856->127.0.0.1:32768: read: connection reset by peer
I0726 10:49:01.647917    6652 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0726 10:49:01.647978    6652 ubuntu.go:169] provisioning hostname "minikube"
I0726 10:49:01.648363    6652 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0726 10:49:01.683963    6652 main.go:141] libmachine: Using SSH client type: native
I0726 10:49:01.684177    6652 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0726 10:49:01.684184    6652 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0726 10:49:01.913639    6652 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0726 10:49:01.913762    6652 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0726 10:49:01.941584    6652 main.go:141] libmachine: Using SSH client type: native
I0726 10:49:01.941826    6652 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0726 10:49:01.941837    6652 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0726 10:49:02.116250    6652 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0726 10:49:02.116353    6652 ubuntu.go:175] set auth options {CertDir:/home/ken/.minikube CaCertPath:/home/ken/.minikube/certs/ca.pem CaPrivateKeyPath:/home/ken/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/ken/.minikube/machines/server.pem ServerKeyPath:/home/ken/.minikube/machines/server-key.pem ClientKeyPath:/home/ken/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/ken/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/ken/.minikube}
I0726 10:49:02.116421    6652 ubuntu.go:177] setting up certificates
I0726 10:49:02.116483    6652 provision.go:84] configureAuth start
I0726 10:49:02.116721    6652 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0726 10:49:02.157513    6652 provision.go:143] copyHostCerts
I0726 10:49:02.159525    6652 exec_runner.go:144] found /home/ken/.minikube/ca.pem, removing ...
I0726 10:49:02.159541    6652 exec_runner.go:203] rm: /home/ken/.minikube/ca.pem
I0726 10:49:02.159786    6652 exec_runner.go:151] cp: /home/ken/.minikube/certs/ca.pem --> /home/ken/.minikube/ca.pem (1070 bytes)
I0726 10:49:02.160135    6652 exec_runner.go:144] found /home/ken/.minikube/cert.pem, removing ...
I0726 10:49:02.160141    6652 exec_runner.go:203] rm: /home/ken/.minikube/cert.pem
I0726 10:49:02.160173    6652 exec_runner.go:151] cp: /home/ken/.minikube/certs/cert.pem --> /home/ken/.minikube/cert.pem (1115 bytes)
I0726 10:49:02.160429    6652 exec_runner.go:144] found /home/ken/.minikube/key.pem, removing ...
I0726 10:49:02.160434    6652 exec_runner.go:203] rm: /home/ken/.minikube/key.pem
I0726 10:49:02.160464    6652 exec_runner.go:151] cp: /home/ken/.minikube/certs/key.pem --> /home/ken/.minikube/key.pem (1679 bytes)
I0726 10:49:02.160700    6652 provision.go:117] generating server cert: /home/ken/.minikube/machines/server.pem ca-key=/home/ken/.minikube/certs/ca.pem private-key=/home/ken/.minikube/certs/ca-key.pem org=ken.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0726 10:49:02.401139    6652 provision.go:177] copyRemoteCerts
I0726 10:49:02.401222    6652 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0726 10:49:02.401255    6652 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0726 10:49:02.420994    6652 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/ken/.minikube/machines/minikube/id_rsa Username:docker}
I0726 10:49:02.555111    6652 ssh_runner.go:362] scp /home/ken/.minikube/machines/server.pem --> /etc/docker/server.pem (1172 bytes)
I0726 10:49:02.594372    6652 ssh_runner.go:362] scp /home/ken/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0726 10:49:02.626699    6652 ssh_runner.go:362] scp /home/ken/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1070 bytes)
I0726 10:49:02.654430    6652 provision.go:87] duration metric: took 537.930811ms to configureAuth
I0726 10:49:02.654444    6652 ubuntu.go:193] setting minikube options for container-runtime
I0726 10:49:02.654676    6652 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0726 10:49:02.654727    6652 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0726 10:49:02.674114    6652 main.go:141] libmachine: Using SSH client type: native
I0726 10:49:02.674358    6652 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0726 10:49:02.674369    6652 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0726 10:49:02.853322    6652 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0726 10:49:02.853357    6652 ubuntu.go:71] root file system type: overlay
I0726 10:49:02.853896    6652 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0726 10:49:02.854159    6652 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0726 10:49:02.891363    6652 main.go:141] libmachine: Using SSH client type: native
I0726 10:49:02.891582    6652 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0726 10:49:02.891649    6652 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0726 10:49:03.102517    6652 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0726 10:49:03.102645    6652 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0726 10:49:03.125930    6652 main.go:141] libmachine: Using SSH client type: native
I0726 10:49:03.126162    6652 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0726 10:49:03.126196    6652 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0726 10:49:03.320718    6652 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0726 10:49:03.320765    6652 machine.go:96] duration metric: took 4.911959671s to provisionDockerMachine
I0726 10:49:03.320802    6652 start.go:293] postStartSetup for "minikube" (driver="docker")
I0726 10:49:03.320846    6652 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0726 10:49:03.321138    6652 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0726 10:49:03.321426    6652 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0726 10:49:03.353179    6652 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/ken/.minikube/machines/minikube/id_rsa Username:docker}
I0726 10:49:03.486142    6652 ssh_runner.go:195] Run: cat /etc/os-release
I0726 10:49:03.492649    6652 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0726 10:49:03.492692    6652 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0726 10:49:03.492713    6652 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0726 10:49:03.492721    6652 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0726 10:49:03.492732    6652 filesync.go:126] Scanning /home/ken/.minikube/addons for local assets ...
I0726 10:49:03.494422    6652 filesync.go:126] Scanning /home/ken/.minikube/files for local assets ...
I0726 10:49:03.494633    6652 start.go:296] duration metric: took 173.816409ms for postStartSetup
I0726 10:49:03.494704    6652 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0726 10:49:03.494754    6652 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0726 10:49:03.521915    6652 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/ken/.minikube/machines/minikube/id_rsa Username:docker}
I0726 10:49:03.635897    6652 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0726 10:49:03.652792    6652 fix.go:56] duration metric: took 5.83541337s for fixHost
I0726 10:49:03.652816    6652 start.go:83] releasing machines lock for "minikube", held for 5.835459856s
I0726 10:49:03.652924    6652 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0726 10:49:03.680908    6652 ssh_runner.go:195] Run: cat /version.json
I0726 10:49:03.680951    6652 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0726 10:49:03.680972    6652 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0726 10:49:03.681005    6652 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0726 10:49:03.702103    6652 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/ken/.minikube/machines/minikube/id_rsa Username:docker}
I0726 10:49:03.704167    6652 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/ken/.minikube/machines/minikube/id_rsa Username:docker}
I0726 10:49:04.093587    6652 ssh_runner.go:195] Run: systemctl --version
I0726 10:49:04.120965    6652 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0726 10:49:04.140580    6652 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0726 10:49:04.175169    6652 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0726 10:49:04.175228    6652 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0726 10:49:04.185558    6652 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0726 10:49:04.185575    6652 start.go:495] detecting cgroup driver to use...
I0726 10:49:04.185605    6652 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0726 10:49:04.185708    6652 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0726 10:49:04.204762    6652 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0726 10:49:04.216720    6652 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0726 10:49:04.228185    6652 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0726 10:49:04.228248    6652 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0726 10:49:04.239996    6652 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0726 10:49:04.251378    6652 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0726 10:49:04.262592    6652 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0726 10:49:04.273448    6652 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0726 10:49:04.284160    6652 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0726 10:49:04.295242    6652 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0726 10:49:04.306724    6652 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0726 10:49:04.318399    6652 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0726 10:49:04.329211    6652 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0726 10:49:04.339785    6652 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0726 10:49:04.440535    6652 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0726 10:49:04.568283    6652 start.go:495] detecting cgroup driver to use...
I0726 10:49:04.568316    6652 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0726 10:49:04.568363    6652 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0726 10:49:04.586015    6652 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0726 10:49:04.586072    6652 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0726 10:49:04.601940    6652 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0726 10:49:04.628883    6652 ssh_runner.go:195] Run: which cri-dockerd
I0726 10:49:04.633278    6652 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0726 10:49:04.646616    6652 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0726 10:49:04.672628    6652 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0726 10:49:04.858622    6652 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0726 10:49:05.009218    6652 docker.go:587] configuring docker to use "cgroupfs" as cgroup driver...
I0726 10:49:05.009333    6652 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0726 10:49:05.042450    6652 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I0726 10:49:05.057526    6652 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0726 10:49:05.177274    6652 ssh_runner.go:195] Run: sudo systemctl restart docker
I0726 10:49:05.707026    6652 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0726 10:49:05.720023    6652 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0726 10:49:05.733862    6652 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0726 10:49:05.746485    6652 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0726 10:49:05.878725    6652 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0726 10:49:05.992994    6652 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0726 10:49:06.097750    6652 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0726 10:49:06.113247    6652 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I0726 10:49:06.125776    6652 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0726 10:49:06.252429    6652 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0726 10:49:06.799191    6652 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0726 10:49:06.813976    6652 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0726 10:49:06.814037    6652 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0726 10:49:06.817851    6652 start.go:563] Will wait 60s for crictl version
I0726 10:49:06.817896    6652 ssh_runner.go:195] Run: which crictl
I0726 10:49:06.821530    6652 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0726 10:49:07.049004    6652 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.1.1
RuntimeApiVersion:  v1
I0726 10:49:07.049067    6652 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0726 10:49:07.272887    6652 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0726 10:49:07.314997    6652 out.go:235] 🐳  Preparing Kubernetes v1.33.1 on Docker 28.1.1 ...
I0726 10:49:07.315086    6652 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0726 10:49:07.335048    6652 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0726 10:49:07.338872    6652 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0726 10:49:07.352907    6652 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/ken:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0726 10:49:07.352981    6652 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0726 10:49:07.353027    6652 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0726 10:49:07.376046    6652 docker.go:702] Got preloaded images: -- stdout --
nginx:latest
registry.k8s.io/kube-controller-manager:v1.33.1
registry.k8s.io/kube-scheduler:v1.33.1
registry.k8s.io/kube-apiserver:v1.33.1
registry.k8s.io/kube-proxy:v1.33.1
registry.k8s.io/etcd:3.5.21-0
registry.k8s.io/coredns/coredns:v1.12.0
registry.k8s.io/metrics-server/metrics-server:<none>
registry.k8s.io/pause:3.10
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
registry.k8s.io/e2e-test-images/agnhost:2.39
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0726 10:49:07.376054    6652 docker.go:632] Images already preloaded, skipping extraction
I0726 10:49:07.376123    6652 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0726 10:49:07.399795    6652 docker.go:702] Got preloaded images: -- stdout --
nginx:latest
registry.k8s.io/kube-apiserver:v1.33.1
registry.k8s.io/kube-scheduler:v1.33.1
registry.k8s.io/kube-controller-manager:v1.33.1
registry.k8s.io/kube-proxy:v1.33.1
registry.k8s.io/etcd:3.5.21-0
registry.k8s.io/coredns/coredns:v1.12.0
registry.k8s.io/metrics-server/metrics-server:<none>
registry.k8s.io/pause:3.10
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
registry.k8s.io/e2e-test-images/agnhost:2.39
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0726 10:49:07.399806    6652 cache_images.go:84] Images are preloaded, skipping loading
I0726 10:49:07.399815    6652 kubeadm.go:926] updating node { 192.168.49.2 8443 v1.33.1 docker true true} ...
I0726 10:49:07.399950    6652 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.33.1/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0726 10:49:07.400002    6652 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0726 10:49:07.779108    6652 cni.go:84] Creating CNI manager for ""
I0726 10:49:07.779133    6652 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0726 10:49:07.779145    6652 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0726 10:49:07.779181    6652 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.33.1 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0726 10:49:07.779385    6652 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.33.1
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0726 10:49:07.779521    6652 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.33.1
I0726 10:49:07.795129    6652 binaries.go:44] Found k8s binaries, skipping transfer
I0726 10:49:07.795199    6652 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0726 10:49:07.810349    6652 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0726 10:49:07.841485    6652 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0726 10:49:07.871525    6652 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0726 10:49:07.896332    6652 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0726 10:49:07.901381    6652 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0726 10:49:07.915075    6652 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0726 10:49:08.050186    6652 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0726 10:49:08.067095    6652 certs.go:68] Setting up /home/ken/.minikube/profiles/minikube for IP: 192.168.49.2
I0726 10:49:08.067103    6652 certs.go:194] generating shared ca certs ...
I0726 10:49:08.067115    6652 certs.go:226] acquiring lock for ca certs: {Name:mk12f21b31a5df941f6933bf526003d4c5e83a24 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0726 10:49:08.067336    6652 certs.go:235] skipping valid "minikubeCA" ca cert: /home/ken/.minikube/ca.key
I0726 10:49:08.067646    6652 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/ken/.minikube/proxy-client-ca.key
I0726 10:49:08.067659    6652 certs.go:256] generating profile certs ...
I0726 10:49:08.067818    6652 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /home/ken/.minikube/profiles/minikube/client.key
I0726 10:49:08.068044    6652 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /home/ken/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0726 10:49:08.068356    6652 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /home/ken/.minikube/profiles/minikube/proxy-client.key
I0726 10:49:08.068589    6652 certs.go:484] found cert: /home/ken/.minikube/certs/ca-key.pem (1671 bytes)
I0726 10:49:08.068639    6652 certs.go:484] found cert: /home/ken/.minikube/certs/ca.pem (1070 bytes)
I0726 10:49:08.068678    6652 certs.go:484] found cert: /home/ken/.minikube/certs/cert.pem (1115 bytes)
I0726 10:49:08.068712    6652 certs.go:484] found cert: /home/ken/.minikube/certs/key.pem (1679 bytes)
I0726 10:49:08.071739    6652 ssh_runner.go:362] scp /home/ken/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0726 10:49:08.111465    6652 ssh_runner.go:362] scp /home/ken/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0726 10:49:08.154604    6652 ssh_runner.go:362] scp /home/ken/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0726 10:49:08.192515    6652 ssh_runner.go:362] scp /home/ken/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0726 10:49:08.232073    6652 ssh_runner.go:362] scp /home/ken/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0726 10:49:08.274599    6652 ssh_runner.go:362] scp /home/ken/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0726 10:49:08.314563    6652 ssh_runner.go:362] scp /home/ken/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0726 10:49:08.346637    6652 ssh_runner.go:362] scp /home/ken/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0726 10:49:08.379155    6652 ssh_runner.go:362] scp /home/ken/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0726 10:49:08.411332    6652 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0726 10:49:08.435132    6652 ssh_runner.go:195] Run: openssl version
I0726 10:49:08.448370    6652 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0726 10:49:08.462380    6652 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0726 10:49:08.468025    6652 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Feb 19 05:51 /usr/share/ca-certificates/minikubeCA.pem
I0726 10:49:08.468200    6652 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0726 10:49:08.477199    6652 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0726 10:49:08.496154    6652 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0726 10:49:08.504490    6652 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0726 10:49:08.514092    6652 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0726 10:49:08.524878    6652 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0726 10:49:08.536606    6652 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0726 10:49:08.547754    6652 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0726 10:49:08.560471    6652 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0726 10:49:08.575028    6652 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/ken:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0726 10:49:08.575158    6652 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0726 10:49:08.657595    6652 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0726 10:49:08.677415    6652 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0726 10:49:08.677425    6652 kubeadm.go:589] restartPrimaryControlPlane start ...
I0726 10:49:08.677478    6652 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0726 10:49:08.732916    6652 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0726 10:49:08.734038    6652 kubeconfig.go:125] found "minikube" server: "https://192.168.49.2:8443"
I0726 10:49:08.762163    6652 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0726 10:49:08.778757    6652 kubeadm.go:626] The running cluster does not require reconfiguration: 192.168.49.2
I0726 10:49:08.778783    6652 kubeadm.go:593] duration metric: took 101.352408ms to restartPrimaryControlPlane
I0726 10:49:08.778791    6652 kubeadm.go:394] duration metric: took 203.781756ms to StartCluster
I0726 10:49:08.778811    6652 settings.go:142] acquiring lock: {Name:mkb4fa576e7fe142bb783e2de0f82914e7defce7 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0726 10:49:08.778908    6652 settings.go:150] Updating kubeconfig:  /home/ken/.kube/config
I0726 10:49:08.779629    6652 lock.go:35] WriteFile acquiring /home/ken/.kube/config: {Name:mk1b4c05d847415f21a9a19940408d92c79ab350 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0726 10:49:08.779982    6652 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}
I0726 10:49:08.780210    6652 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:true nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0726 10:49:08.780315    6652 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0726 10:49:08.780332    6652 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W0726 10:49:08.780339    6652 addons.go:247] addon storage-provisioner should already be in state true
I0726 10:49:08.780376    6652 host.go:66] Checking if "minikube" exists ...
I0726 10:49:08.780639    6652 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0726 10:49:08.780719    6652 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0726 10:49:08.780746    6652 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0726 10:49:08.781004    6652 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0726 10:49:08.781207    6652 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0726 10:49:08.781206    6652 addons.go:69] Setting metrics-server=true in profile "minikube"
I0726 10:49:08.781227    6652 addons.go:238] Setting addon metrics-server=true in "minikube"
W0726 10:49:08.781236    6652 addons.go:247] addon metrics-server should already be in state true
I0726 10:49:08.781294    6652 host.go:66] Checking if "minikube" exists ...
I0726 10:49:08.782193    6652 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0726 10:49:08.785498    6652 addons.go:69] Setting dashboard=true in profile "minikube"
I0726 10:49:08.785526    6652 addons.go:238] Setting addon dashboard=true in "minikube"
W0726 10:49:08.785538    6652 addons.go:247] addon dashboard should already be in state true
I0726 10:49:08.785581    6652 host.go:66] Checking if "minikube" exists ...
I0726 10:49:08.786192    6652 out.go:177] 🔎  Verifying Kubernetes components...
I0726 10:49:08.786396    6652 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0726 10:49:08.787756    6652 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0726 10:49:08.880802    6652 out.go:177]     ▪ Using image docker.io/kubernetesui/dashboard:v2.7.0
I0726 10:49:08.884236    6652 out.go:177]     ▪ Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
I0726 10:49:08.887224    6652 addons.go:435] installing /etc/kubernetes/addons/dashboard-ns.yaml
I0726 10:49:08.887258    6652 ssh_runner.go:362] scp dashboard/dashboard-ns.yaml --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I0726 10:49:08.887421    6652 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0726 10:49:08.895554    6652 out.go:177]     ▪ Using image registry.k8s.io/metrics-server/metrics-server:v0.7.2
I0726 10:49:08.896973    6652 addons.go:435] installing /etc/kubernetes/addons/metrics-apiservice.yaml
I0726 10:49:08.897004    6652 ssh_runner.go:362] scp metrics-server/metrics-apiservice.yaml --> /etc/kubernetes/addons/metrics-apiservice.yaml (424 bytes)
I0726 10:49:08.897222    6652 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0726 10:49:08.898668    6652 addons.go:238] Setting addon default-storageclass=true in "minikube"
W0726 10:49:08.898699    6652 addons.go:247] addon default-storageclass should already be in state true
I0726 10:49:08.898764    6652 host.go:66] Checking if "minikube" exists ...
I0726 10:49:08.899726    6652 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0726 10:49:08.941279    6652 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0726 10:49:08.946469    6652 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0726 10:49:08.946510    6652 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0726 10:49:08.946690    6652 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0726 10:49:09.006093    6652 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/ken/.minikube/machines/minikube/id_rsa Username:docker}
I0726 10:49:09.041006    6652 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/ken/.minikube/machines/minikube/id_rsa Username:docker}
I0726 10:49:09.043091    6652 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0726 10:49:09.043103    6652 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0726 10:49:09.043168    6652 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0726 10:49:09.072818    6652 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0726 10:49:09.075017    6652 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/ken/.minikube/machines/minikube/id_rsa Username:docker}
I0726 10:49:09.128587    6652 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/ken/.minikube/machines/minikube/id_rsa Username:docker}
I0726 10:49:09.155932    6652 api_server.go:52] waiting for apiserver process to appear ...
I0726 10:49:09.156015    6652 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0726 10:49:09.467025    6652 addons.go:435] installing /etc/kubernetes/addons/metrics-server-deployment.yaml
I0726 10:49:09.467041    6652 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-deployment.yaml (1907 bytes)
I0726 10:49:09.471851    6652 addons.go:435] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I0726 10:49:09.471868    6652 ssh_runner.go:362] scp dashboard/dashboard-clusterrole.yaml --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I0726 10:49:09.584856    6652 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0726 10:49:09.645990    6652 addons.go:435] installing /etc/kubernetes/addons/metrics-server-rbac.yaml
I0726 10:49:09.646006    6652 ssh_runner.go:362] scp metrics-server/metrics-server-rbac.yaml --> /etc/kubernetes/addons/metrics-server-rbac.yaml (2175 bytes)
I0726 10:49:09.656955    6652 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0726 10:49:09.657863    6652 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0726 10:49:09.679975    6652 addons.go:435] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I0726 10:49:09.679990    6652 ssh_runner.go:362] scp dashboard/dashboard-clusterrolebinding.yaml --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I0726 10:49:09.783896    6652 addons.go:435] installing /etc/kubernetes/addons/metrics-server-service.yaml
I0726 10:49:09.783913    6652 ssh_runner.go:362] scp metrics-server/metrics-server-service.yaml --> /etc/kubernetes/addons/metrics-server-service.yaml (446 bytes)
I0726 10:49:10.039213    6652 addons.go:435] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I0726 10:49:10.039255    6652 ssh_runner.go:362] scp dashboard/dashboard-configmap.yaml --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I0726 10:49:10.069802    6652 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml
I0726 10:49:10.271772    6652 addons.go:435] installing /etc/kubernetes/addons/dashboard-dp.yaml
I0726 10:49:10.271787    6652 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I0726 10:49:10.529000    6652 addons.go:435] installing /etc/kubernetes/addons/dashboard-role.yaml
I0726 10:49:10.529033    6652 ssh_runner.go:362] scp dashboard/dashboard-role.yaml --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I0726 10:49:10.783720    6652 addons.go:435] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I0726 10:49:10.783740    6652 ssh_runner.go:362] scp dashboard/dashboard-rolebinding.yaml --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I0726 10:49:11.019748    6652 addons.go:435] installing /etc/kubernetes/addons/dashboard-sa.yaml
I0726 10:49:11.019772    6652 ssh_runner.go:362] scp dashboard/dashboard-sa.yaml --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I0726 10:49:11.271065    6652 addons.go:435] installing /etc/kubernetes/addons/dashboard-secret.yaml
I0726 10:49:11.271078    6652 ssh_runner.go:362] scp dashboard/dashboard-secret.yaml --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I0726 10:49:11.281330    6652 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.696429348s)
W0726 10:49:11.281358    6652 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0726 10:49:11.281378    6652 retry.go:31] will retry after 298.437105ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0726 10:49:11.281410    6652 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (1.624441749s)
I0726 10:49:11.281455    6652 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0726 10:49:11.473987    6652 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (1.81609969s)
W0726 10:49:11.474011    6652 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0726 10:49:11.474025    6652 retry.go:31] will retry after 191.15198ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0726 10:49:11.474197    6652 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: (1.404377546s)
W0726 10:49:11.474220    6652 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/metrics-apiservice.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-deployment.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-rbac.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-service.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0726 10:49:11.474228    6652 retry.go:31] will retry after 159.51419ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/metrics-apiservice.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-deployment.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-rbac.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-service.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0726 10:49:11.479665    6652 addons.go:435] installing /etc/kubernetes/addons/dashboard-svc.yaml
I0726 10:49:11.479704    6652 ssh_runner.go:362] scp dashboard/dashboard-svc.yaml --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I0726 10:49:11.580447    6652 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0726 10:49:11.585627    6652 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0726 10:49:11.634088    6652 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml
I0726 10:49:11.657137    6652 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0726 10:49:11.666209    6652 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0726 10:49:12.241137    6652 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0726 10:49:12.241155    6652 retry.go:31] will retry after 208.850114ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0726 10:49:12.241753    6652 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0726 10:49:12.241764    6652 retry.go:31] will retry after 266.305954ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0726 10:49:12.244339    6652 api_server.go:72] duration metric: took 3.464330335s to wait for apiserver process to appear ...
I0726 10:49:12.244359    6652 api_server.go:88] waiting for apiserver healthz status ...
I0726 10:49:12.244383    6652 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
W0726 10:49:12.244734    6652 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/metrics-apiservice.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-deployment.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-rbac.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-service.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0726 10:49:12.244749    6652 retry.go:31] will retry after 543.330135ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/metrics-apiservice.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-deployment.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-rbac.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-service.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0726 10:49:12.244877    6652 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
W0726 10:49:12.283875    6652 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0726 10:49:12.283894    6652 retry.go:31] will retry after 255.262113ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0726 10:49:12.450111    6652 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0726 10:49:12.508470    6652 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0726 10:49:12.540213    6652 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0726 10:49:12.629139    6652 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0726 10:49:12.629156    6652 retry.go:31] will retry after 433.447481ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0726 10:49:12.744507    6652 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0726 10:49:12.788566    6652 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml
I0726 10:49:13.063212    6652 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0726 10:49:17.442863    6652 api_server.go:279] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0726 10:49:17.442881    6652 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0726 10:49:17.442896    6652 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0726 10:49:17.569521    6652 api_server.go:279] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0726 10:49:17.569542    6652 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0726 10:49:17.744929    6652 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0726 10:49:17.778269    6652 api_server.go:279] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\": RBAC: clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found","reason":"Forbidden","details":{},"code":403}
W0726 10:49:17.778319    6652 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\": RBAC: clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found","reason":"Forbidden","details":{},"code":403}
I0726 10:49:18.244818    6652 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0726 10:49:18.258288    6652 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0726 10:49:18.258362    6652 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0726 10:49:18.745161    6652 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0726 10:49:18.792571    6652 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0726 10:49:18.792593    6652 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0726 10:49:19.245332    6652 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0726 10:49:19.332023    6652 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0726 10:49:19.332075    6652 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0726 10:49:19.744566    6652 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0726 10:49:19.831186    6652 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0726 10:49:19.831248    6652 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0726 10:49:20.247999    6652 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0726 10:49:20.258088    6652 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0726 10:49:20.258106    6652 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0726 10:49:20.744510    6652 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0726 10:49:20.759924    6652 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0726 10:49:20.759945    6652 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0726 10:49:21.244891    6652 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0726 10:49:21.271253    6652 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0726 10:49:21.271319    6652 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0726 10:49:21.744968    6652 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0726 10:49:21.750976    6652 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0726 10:49:21.750996    6652 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0726 10:49:22.244584    6652 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0726 10:49:22.252586    6652 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0726 10:49:22.252609    6652 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0726 10:49:22.745016    6652 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0726 10:49:22.752760    6652 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0726 10:49:22.752778    6652 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0726 10:49:23.245374    6652 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0726 10:49:23.267186    6652 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I0726 10:49:23.283368    6652 api_server.go:141] control plane version: v1.33.1
I0726 10:49:23.283390    6652 api_server.go:131] duration metric: took 11.039021284s to wait for apiserver health ...
I0726 10:49:23.283408    6652 system_pods.go:43] waiting for kube-system pods to appear ...
I0726 10:49:23.301842    6652 system_pods.go:59] 8 kube-system pods found
I0726 10:49:23.302004    6652 system_pods.go:61] "coredns-674b8bbfcf-flhf9" [f960a04b-38b1-4924-b446-575c359df649] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0726 10:49:23.302040    6652 system_pods.go:61] "etcd-minikube" [9a61b796-2fef-4f4e-8d70-ff44f73a6565] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0726 10:49:23.302061    6652 system_pods.go:61] "kube-apiserver-minikube" [e7f90063-0449-4335-9547-eb3810a08c09] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0726 10:49:23.302100    6652 system_pods.go:61] "kube-controller-manager-minikube" [54c21f10-3a7e-465c-b764-715ffe7b2576] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0726 10:49:23.302119    6652 system_pods.go:61] "kube-proxy-hzrth" [2cb21360-70bb-4ee8-a0ec-ec86d8b24a1e] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0726 10:49:23.302130    6652 system_pods.go:61] "kube-scheduler-minikube" [00ce793a-dac3-446f-aaad-5c005bf7af7e] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0726 10:49:23.302155    6652 system_pods.go:61] "metrics-server-7fbb699795-6dn7t" [95b882ea-6bb2-46e3-812e-169ab8fdaee6] Running / Ready:ContainersNotReady (containers with unready status: [metrics-server]) / ContainersReady:ContainersNotReady (containers with unready status: [metrics-server])
I0726 10:49:23.302166    6652 system_pods.go:61] "storage-provisioner" [9fbdf6ee-403b-43f3-8ed7-9ccc9e6f2ec9] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0726 10:49:23.302180    6652 system_pods.go:74] duration metric: took 18.76211ms to wait for pod list to return data ...
I0726 10:49:23.302210    6652 kubeadm.go:578] duration metric: took 14.522203786s to wait for: map[apiserver:true system_pods:true]
I0726 10:49:23.302254    6652 node_conditions.go:102] verifying NodePressure condition ...
I0726 10:49:23.309603    6652 node_conditions.go:122] node storage ephemeral capacity is 244506940Ki
I0726 10:49:23.309624    6652 node_conditions.go:123] node cpu capacity is 4
I0726 10:49:23.309647    6652 node_conditions.go:105] duration metric: took 7.387803ms to run NodePressure ...
I0726 10:49:23.309661    6652 start.go:241] waiting for startup goroutines ...
I0726 10:49:28.446945    6652 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (15.938394664s)
I0726 10:49:28.446980    6652 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (15.906745382s)
I0726 10:49:28.447055    6652 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: (15.658464074s)
I0726 10:49:28.447073    6652 addons.go:479] Verifying addon metrics-server=true in "minikube"
I0726 10:49:28.447147    6652 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (15.383918364s)
I0726 10:49:28.449097    6652 out.go:177] 💡  Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server

I0726 10:49:28.452358    6652 out.go:177] 🌟  Enabled addons: metrics-server, storage-provisioner, dashboard, default-storageclass
I0726 10:49:28.453157    6652 addons.go:514] duration metric: took 19.672955576s for enable addons: enabled=[metrics-server storage-provisioner dashboard default-storageclass]
I0726 10:49:28.453184    6652 start.go:246] waiting for cluster config update ...
I0726 10:49:28.453197    6652 start.go:255] writing updated cluster config ...
I0726 10:49:28.453491    6652 ssh_runner.go:195] Run: rm -f paused
I0726 10:49:28.533771    6652 start.go:607] kubectl: 1.33.3, cluster: 1.33.1 (minor skew: 0)
I0726 10:49:28.535018    6652 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Jul 26 05:19:05 minikube dockerd[1177]: time="2025-07-26T05:19:05.236447242Z" level=info msg="OTEL tracing is not configured, using no-op tracer provider"
Jul 26 05:19:05 minikube dockerd[1177]: time="2025-07-26T05:19:05.247407793Z" level=info msg="Creating a containerd client" address=/run/containerd/containerd.sock timeout=1m0s
Jul 26 05:19:05 minikube dockerd[1177]: time="2025-07-26T05:19:05.254229819Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Jul 26 05:19:05 minikube dockerd[1177]: time="2025-07-26T05:19:05.273073503Z" level=info msg="Loading containers: start."
Jul 26 05:19:05 minikube dockerd[1177]: time="2025-07-26T05:19:05.620118382Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint_count 05d795c86822000a238f8461a656a16781182820c7b350df5b56a2dd5bed3c39], retrying...."
Jul 26 05:19:05 minikube dockerd[1177]: time="2025-07-26T05:19:05.662060157Z" level=info msg="Loading containers: done."
Jul 26 05:19:05 minikube dockerd[1177]: time="2025-07-26T05:19:05.673768336Z" level=info msg="Docker daemon" commit=01f442b containerd-snapshotter=false storage-driver=overlay2 version=28.1.1
Jul 26 05:19:05 minikube dockerd[1177]: time="2025-07-26T05:19:05.673846605Z" level=info msg="Initializing buildkit"
Jul 26 05:19:05 minikube dockerd[1177]: time="2025-07-26T05:19:05.697605522Z" level=info msg="Completed buildkit initialization"
Jul 26 05:19:05 minikube dockerd[1177]: time="2025-07-26T05:19:05.704746167Z" level=info msg="Daemon has completed initialization"
Jul 26 05:19:05 minikube dockerd[1177]: time="2025-07-26T05:19:05.704801577Z" level=info msg="API listen on /var/run/docker.sock"
Jul 26 05:19:05 minikube dockerd[1177]: time="2025-07-26T05:19:05.704851457Z" level=info msg="API listen on [::]:2376"
Jul 26 05:19:05 minikube systemd[1]: Started Docker Application Container Engine.
Jul 26 05:19:06 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Jul 26 05:19:06 minikube cri-dockerd[1484]: time="2025-07-26T05:19:06Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Jul 26 05:19:06 minikube cri-dockerd[1484]: time="2025-07-26T05:19:06Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Jul 26 05:19:06 minikube cri-dockerd[1484]: time="2025-07-26T05:19:06Z" level=info msg="Start docker client with request timeout 0s"
Jul 26 05:19:06 minikube cri-dockerd[1484]: time="2025-07-26T05:19:06Z" level=info msg="Hairpin mode is set to hairpin-veth"
Jul 26 05:19:06 minikube cri-dockerd[1484]: time="2025-07-26T05:19:06Z" level=info msg="Loaded network plugin cni"
Jul 26 05:19:06 minikube cri-dockerd[1484]: time="2025-07-26T05:19:06Z" level=info msg="Docker cri networking managed by network plugin cni"
Jul 26 05:19:06 minikube cri-dockerd[1484]: time="2025-07-26T05:19:06Z" level=info msg="Setting cgroupDriver cgroupfs"
Jul 26 05:19:06 minikube cri-dockerd[1484]: time="2025-07-26T05:19:06Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Jul 26 05:19:06 minikube cri-dockerd[1484]: time="2025-07-26T05:19:06Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Jul 26 05:19:06 minikube cri-dockerd[1484]: time="2025-07-26T05:19:06Z" level=info msg="Start cri-dockerd grpc backend"
Jul 26 05:19:06 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Jul 26 05:19:08 minikube cri-dockerd[1484]: time="2025-07-26T05:19:08Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"metrics-server-7fbb699795-6dn7t_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"fb7757038e5c1dc616601979f7925e7fc255c78bc5c9f57b06f33ea2eba04f48\""
Jul 26 05:19:08 minikube cri-dockerd[1484]: time="2025-07-26T05:19:08Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-674b8bbfcf-flhf9_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"94ffe3b4c567754b26b81a764774a264b4257428019f346fb933cd939e46b860\""
Jul 26 05:19:08 minikube cri-dockerd[1484]: time="2025-07-26T05:19:08Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-674b8bbfcf-flhf9_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"dd559df98f9b4fcbd01104193d3041155436a3449872ac981302fe54574e760f\""
Jul 26 05:19:09 minikube cri-dockerd[1484]: time="2025-07-26T05:19:09Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"hello-node-c74958b5d-r58j7_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"7ba1ef7364782b9d5ac827de98a5d9199bc8369e579794503bff39df1b76462b\""
Jul 26 05:19:09 minikube cri-dockerd[1484]: time="2025-07-26T05:19:09Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-5d59dccf9b-4gd75_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"a40d64f27631ebbe948b6e9f2ac84c6deec8a0f2412ebec3fc15ea3a9b44045d\""
Jul 26 05:19:09 minikube cri-dockerd[1484]: time="2025-07-26T05:19:09Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"kubernetes-dashboard-7779f9b69b-ggp6t_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"94380c23f18a9df1c95284e6f4ad4d72833367bc9c9423d1bda5db9b6ea48058\""
Jul 26 05:19:09 minikube cri-dockerd[1484]: time="2025-07-26T05:19:09Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"nginx_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"369263e9149ee9e3c17d23cf194d6e0ffe814503cc57c7ce5ca516ed94056a28\""
Jul 26 05:19:09 minikube cri-dockerd[1484]: time="2025-07-26T05:19:09Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"nginx_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"0a639ae0137f44e2da16879d4683275caccc2526f56fc65e9393744c8a092d92\""
Jul 26 05:19:10 minikube cri-dockerd[1484]: time="2025-07-26T05:19:10Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/fea4e7bb5edcc797c6bdf827f625c0e8567e7906b0a95d2512893ca13d6e9020/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Jul 26 05:19:10 minikube cri-dockerd[1484]: time="2025-07-26T05:19:10Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0bb1008e9b29844eefb872e5c0929f48cfb8edaedd2214071b7cb058b81260ba/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Jul 26 05:19:10 minikube cri-dockerd[1484]: time="2025-07-26T05:19:10Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c48e48375e100636d506979bdd215768ae4ca1660d7ce63c4b1ab1c4285bddf1/resolv.conf as [nameserver 192.168.49.1 options ndots:0 edns0 trust-ad]"
Jul 26 05:19:10 minikube cri-dockerd[1484]: time="2025-07-26T05:19:10Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9c9e5a89033f1d7020382ffd613faf19468a3ae2273db6418e6f92a80d700fca/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Jul 26 05:19:11 minikube cri-dockerd[1484]: time="2025-07-26T05:19:11Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-674b8bbfcf-flhf9_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"94ffe3b4c567754b26b81a764774a264b4257428019f346fb933cd939e46b860\""
Jul 26 05:19:12 minikube cri-dockerd[1484]: time="2025-07-26T05:19:12Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"nginx_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"369263e9149ee9e3c17d23cf194d6e0ffe814503cc57c7ce5ca516ed94056a28\""
Jul 26 05:19:17 minikube cri-dockerd[1484]: time="2025-07-26T05:19:17Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Jul 26 05:19:20 minikube cri-dockerd[1484]: time="2025-07-26T05:19:20Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a8b93f3cc78e1ede5907b84aa63ccc1292f5ef5f4e620907f1f931b5398e2652/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 26 05:19:21 minikube cri-dockerd[1484]: time="2025-07-26T05:19:21Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d10e14768f5d7b6d7030a27feccbe4166a6699254a25c7466e25239ba4f56dfd/resolv.conf as [nameserver 10.96.0.10 search kube-system.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 26 05:19:21 minikube cri-dockerd[1484]: time="2025-07-26T05:19:21Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1076d9a8cec67f2d180f8522d45612750270013ded6561614567dde80178414a/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 26 05:19:22 minikube cri-dockerd[1484]: time="2025-07-26T05:19:22Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7c807b4156b8aefb80dcb3509ee45b927395067b318b8196c736d1a2c1525e35/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Jul 26 05:19:22 minikube cri-dockerd[1484]: time="2025-07-26T05:19:22Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1e33a8e265998a3fb06041c9a37b4f466be3439da6355dad50dcb8075df4a51e/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 26 05:19:22 minikube cri-dockerd[1484]: time="2025-07-26T05:19:22Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/046308db68868b062ab180b27baa469fd9009cac252a67b74688cafe9b9e4675/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Jul 26 05:19:23 minikube cri-dockerd[1484]: time="2025-07-26T05:19:23Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ee4a937cb8a0b04d7def2b6e5dc711d4e67f6d7febda76952c360fc81296b9c0/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 26 05:19:23 minikube cri-dockerd[1484]: time="2025-07-26T05:19:23Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/cc650b615c1de1216b40d08cd6fd07a70257fa64d9b0766e4465213048f940a0/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Jul 26 05:19:26 minikube cri-dockerd[1484]: time="2025-07-26T05:19:26Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Jul 26 05:19:55 minikube dockerd[1177]: time="2025-07-26T05:19:55.245855395Z" level=info msg="ignoring event" container=bfadcc763bf34beca69d36b66b5ab1826f3796b316a2b7f5469e3eaa8e7d6b27 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 26 05:19:56 minikube dockerd[1177]: time="2025-07-26T05:19:56.618504148Z" level=info msg="ignoring event" container=2ca667839b52dc27854468effaeb49d111ef7e9ee485b70d4d38ac7d7ea83f1e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 26 05:19:57 minikube dockerd[1177]: time="2025-07-26T05:19:57.162048828Z" level=info msg="ignoring event" container=ef59affb95b21f9819a76058c9fe2cf8ac6f90e4b1b948e8e990b2252eddb714 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 26 06:02:26 minikube dockerd[1177]: time="2025-07-26T06:02:26.192571596Z" level=info msg="ignoring event" container=c1e46574662d4d594214ee6691b1e832b3b438203e0a6fe3d3cd01163e4a0250 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 26 06:02:26 minikube dockerd[1177]: time="2025-07-26T06:02:26.468590557Z" level=info msg="ignoring event" container=1076d9a8cec67f2d180f8522d45612750270013ded6561614567dde80178414a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 26 07:28:47 minikube cri-dockerd[1484]: time="2025-07-26T07:28:47Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f9d6c38cb47c580180214ad98829d9bb4a3134cb9ccf21998d13a755949cbb84/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 26 07:28:50 minikube cri-dockerd[1484]: time="2025-07-26T07:28:50Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Jul 26 07:41:51 minikube cri-dockerd[1484]: time="2025-07-26T07:41:51Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/73fd17532554391f1fe8bd3bca4cf21e5e641d4d2d3e567b81531a2775a1d860/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 26 07:41:51 minikube cri-dockerd[1484]: time="2025-07-26T07:41:51Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4285b55eda80ddb80b9d0e03d7738fb21bcffaa3289e7794ab45465d7a6f23af/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 26 07:41:54 minikube cri-dockerd[1484]: time="2025-07-26T07:41:54Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Jul 26 07:41:57 minikube cri-dockerd[1484]: time="2025-07-26T07:41:57Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"


==> container status <==
CONTAINER           IMAGE                                                                                                             CREATED             STATE               NAME                        ATTEMPT             POD ID              POD
0d7a18589e5cf       nginx@sha256:84ec966e61a8c7846f509da7eb081c55c1d56817448728924a87ab32f12a72fb                                     57 minutes ago      Running             nginx                       0                   4285b55eda80d       nginx-deployment-6858ccbc5-4x7d5
7a116509e8532       nginx@sha256:84ec966e61a8c7846f509da7eb081c55c1d56817448728924a87ab32f12a72fb                                     57 minutes ago      Running             nginx                       0                   73fd175325543       nginx-deployment-6858ccbc5-r679z
9d83ef99f87b7       nginx@sha256:84ec966e61a8c7846f509da7eb081c55c1d56817448728924a87ab32f12a72fb                                     About an hour ago   Running             nginx                       0                   f9d6c38cb47c5       nginx-pod
f88ddf17cb733       07655ddf2eebe                                                                                                     3 hours ago         Running             kubernetes-dashboard        2                   ee4a937cb8a0b       kubernetes-dashboard-7779f9b69b-ggp6t
0042ffbaf8305       6e38f40d628db                                                                                                     3 hours ago         Running             storage-provisioner         8                   046308db68868       storage-provisioner
8f63a7e6d528a       48d9cfaaf3904                                                                                                     3 hours ago         Running             metrics-server              2                   d10e14768f5d7       metrics-server-7fbb699795-6dn7t
ef59affb95b21       07655ddf2eebe                                                                                                     3 hours ago         Exited              kubernetes-dashboard        1                   ee4a937cb8a0b       kubernetes-dashboard-7779f9b69b-ggp6t
720021738a1b7       115053965e86b                                                                                                     3 hours ago         Running             dashboard-metrics-scraper   1                   1e33a8e265998       dashboard-metrics-scraper-5d59dccf9b-4gd75
c72d0d6233c2c       1cf5f116067c6                                                                                                     3 hours ago         Running             coredns                     4                   7c807b4156b8a       coredns-674b8bbfcf-flhf9
eba12fda0211c       b79c189b052cd                                                                                                     3 hours ago         Running             kube-proxy                  4                   cc650b615c1de       kube-proxy-hzrth
bfadcc763bf34       6e38f40d628db                                                                                                     3 hours ago         Exited              storage-provisioner         7                   046308db68868       storage-provisioner
2ca667839b52d       48d9cfaaf3904                                                                                                     3 hours ago         Exited              metrics-server              1                   d10e14768f5d7       metrics-server-7fbb699795-6dn7t
eab50daa87173       a05bd3a9140b7                                                                                                     3 hours ago         Running             agnhost                     1                   a8b93f3cc78e1       hello-node-c74958b5d-r58j7
0aec3ec189bf9       398c985c0d950                                                                                                     3 hours ago         Running             kube-scheduler              4                   9c9e5a89033f1       kube-scheduler-minikube
085901a5cfa07       ef43894fa110c                                                                                                     3 hours ago         Running             kube-controller-manager     5                   c48e48375e100       kube-controller-manager-minikube
71c7de7e34ec5       c6ab243b29f82                                                                                                     3 hours ago         Running             kube-apiserver              4                   0bb1008e9b298       kube-apiserver-minikube
df316af689c80       499038711c081                                                                                                     3 hours ago         Running             etcd                        4                   fea4e7bb5edcc       etcd-minikube
808274bb26da5       registry.k8s.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e   18 hours ago        Exited              agnhost                     0                   7ba1ef7364782       hello-node-c74958b5d-r58j7
937e7343ab410       kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c              18 hours ago        Exited              dashboard-metrics-scraper   0                   a40d64f27631e       dashboard-metrics-scraper-5d59dccf9b-4gd75
b687a8f04a111       1cf5f116067c6                                                                                                     18 hours ago        Exited              coredns                     3                   94ffe3b4c5677       coredns-674b8bbfcf-flhf9
7035a1e0260c8       b79c189b052cd                                                                                                     18 hours ago        Exited              kube-proxy                  3                   873032d42a72e       kube-proxy-hzrth
145da57a89d13       c6ab243b29f82                                                                                                     18 hours ago        Exited              kube-apiserver              3                   ed03ecf62ed9e       kube-apiserver-minikube
e581889c7bd42       ef43894fa110c                                                                                                     18 hours ago        Exited              kube-controller-manager     4                   1ff9004c7bef6       kube-controller-manager-minikube
97021e58e2bc0       499038711c081                                                                                                     18 hours ago        Exited              etcd                        3                   bc015d1b3f9f5       etcd-minikube
95cfd2e0878ca       398c985c0d950                                                                                                     18 hours ago        Exited              kube-scheduler              3                   ef5b578433cd8       kube-scheduler-minikube


==> coredns [b687a8f04a11] <==
maxprocs: Leaving GOMAXPROCS=4: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.12.0
linux/amd64, go1.23.3, 51e11f1
[INFO] 127.0.0.1:39071 - 20831 "HINFO IN 7891233228455362268.2164080810602515591. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.035452612s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> coredns [c72d0d6233c2] <==
maxprocs: Leaving GOMAXPROCS=4: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.12.0
linux/amd64, go1.23.3, 51e11f1
[INFO] 127.0.0.1:48456 - 29023 "HINFO IN 4414639358991378255.6071410729744705043. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.030942782s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=f8f52f5de11fc6ad8244afac475e1d0f96841df1-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_07_24T20_48_29_0700
                    minikube.k8s.io/version=v1.36.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 24 Jul 2025 15:18:26 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sat, 26 Jul 2025 08:39:44 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sat, 26 Jul 2025 08:39:00 +0000   Thu, 24 Jul 2025 15:18:23 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sat, 26 Jul 2025 08:39:00 +0000   Thu, 24 Jul 2025 15:18:23 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sat, 26 Jul 2025 08:39:00 +0000   Thu, 24 Jul 2025 15:18:23 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sat, 26 Jul 2025 08:39:00 +0000   Thu, 24 Jul 2025 15:18:26 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  244506940Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8006036Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  244506940Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8006036Ki
  pods:               110
System Info:
  Machine ID:                 32d9d518d44640cf9d04bbf2cc7a62c3
  System UUID:                6b9c45aa-7c17-4e47-ac62-8f8af46d4ea8
  Boot ID:                    7966a0be-a9b1-4e2a-a6de-092e33db0c7d
  Kernel Version:             5.15.0-139-generic
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.1.1
  Kubelet Version:            v1.33.1
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (14 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  default                     hello-node-c74958b5d-r58j7                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         17h
  default                     nginx-deployment-6858ccbc5-4x7d5              0 (0%)        0 (0%)      0 (0%)           0 (0%)         58m
  default                     nginx-deployment-6858ccbc5-r679z              0 (0%)        0 (0%)      0 (0%)           0 (0%)         58m
  default                     nginx-pod                                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         71m
  kube-system                 coredns-674b8bbfcf-flhf9                      100m (2%)     0 (0%)      70Mi (0%)        170Mi (2%)     41h
  kube-system                 etcd-minikube                                 100m (2%)     0 (0%)      100Mi (1%)       0 (0%)         41h
  kube-system                 kube-apiserver-minikube                       250m (6%)     0 (0%)      0 (0%)           0 (0%)         41h
  kube-system                 kube-controller-manager-minikube              200m (5%)     0 (0%)      0 (0%)           0 (0%)         41h
  kube-system                 kube-proxy-hzrth                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         41h
  kube-system                 kube-scheduler-minikube                       100m (2%)     0 (0%)      0 (0%)           0 (0%)         41h
  kube-system                 metrics-server-7fbb699795-6dn7t               100m (2%)     0 (0%)      200Mi (2%)       0 (0%)         17h
  kube-system                 storage-provisioner                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         41h
  kubernetes-dashboard        dashboard-metrics-scraper-5d59dccf9b-4gd75    0 (0%)        0 (0%)      0 (0%)           0 (0%)         17h
  kubernetes-dashboard        kubernetes-dashboard-7779f9b69b-ggp6t         0 (0%)        0 (0%)      0 (0%)           0 (0%)         17h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (21%)  0 (0%)
  memory             370Mi (4%)  170Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:              <none>


==> dmesg <==
[Jul26 05:27] x86/cpu: SGX disabled by BIOS.
[  +0.006082] MDS CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/mds.html for more details.
[  +0.000000] MMIO Stale Data CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/processor_mmio_stale_data.html for more details.
[  +0.000000]  #3
[  +0.003013] ENERGY_PERF_BIAS: Set to 'normal', was 'performance'
[  +0.289438] hpet_acpi_add: no address or irqs in _CRS
[  +0.053577] i8042: PNP: PS/2 appears to have AUX port disabled, if this is incorrect please boot with i8042.nopnp
[  +0.000272] i8042: Warning: Keylock active
[  +0.005898] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +0.000197] platform eisa.0: EISA: Cannot allocate resource for mainboard
[  +0.000003] platform eisa.0: Cannot allocate resource for EISA slot 1
[  +0.000003] platform eisa.0: Cannot allocate resource for EISA slot 2
[  +0.000002] platform eisa.0: Cannot allocate resource for EISA slot 3
[  +0.000003] platform eisa.0: Cannot allocate resource for EISA slot 4
[  +0.000002] platform eisa.0: Cannot allocate resource for EISA slot 5
[  +0.000003] platform eisa.0: Cannot allocate resource for EISA slot 6
[  +0.000003] platform eisa.0: Cannot allocate resource for EISA slot 7
[  +0.000002] platform eisa.0: Cannot allocate resource for EISA slot 8
[  +0.434137] ACPI Warning: SystemMemory range 0x00000000FE028000-0x00000000FE0281FF conflicts with OpRegion 0x00000000FE028000-0x00000000FE028207 (\_SB.PCI0.GEXP.BAR0) (20210730/utaddress-204)
[  +0.000320] intel-lpss: probe of INT3446:00 failed with error -16
[  +0.056352] wmi_bus wmi_bus-PNP0C14:01: WQBC data block query control method not found
[  +1.693049] systemd[1]: Configuration file /run/systemd/system/netplan-ovs-cleanup.service is marked world-inaccessible. This has no effect as configuration data is accessible via APIs without restrictions. Proceeding anyway.
[  +0.021942] systemd[1]: /lib/systemd/system/snapd.service:23: Unknown key name 'RestartMode' in section 'Service', ignoring.
[  +1.329783] thermal thermal_zone7: failed to read out thermal zone (-61)
[  +1.597376] Bluetooth: hci0: Reading supported features failed (-16)
[  +1.180595] process '/usr/bin/anydesk' started with executable stack
[  +7.733601] kauditd_printk_skb: 63 callbacks suppressed
[Jul26 05:40] tmpfs: Unknown parameter 'noswap'
[Jul26 05:48] done.
[  +0.043487] thermal thermal_zone7: failed to read out thermal zone (-61)
[  +1.434358] Bluetooth: hci0: Reading supported features failed (-16)
[Jul26 08:21] xhci_hcd 0000:00:14.0: xHC error in resume, USBSTS 0x411, Reinit
[  +1.266076] thermal thermal_zone7: failed to read out thermal zone (-61)
[  +1.435800] Bluetooth: hci0: Reading supported features failed (-16)


==> etcd [97021e58e2bc] <==
{"level":"info","ts":"2025-07-25T15:01:25.265577Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":5907,"took":"37.191299ms","hash":2245358680,"current-db-size-bytes":3158016,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1454080,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2025-07-25T15:01:25.265684Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2245358680,"revision":5907,"compact-revision":5040}
{"level":"info","ts":"2025-07-25T15:06:25.235848Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6165}
{"level":"info","ts":"2025-07-25T15:06:25.244035Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":6165,"took":"7.25406ms","hash":3265918224,"current-db-size-bytes":3158016,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1777664,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-07-25T15:06:25.244189Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3265918224,"revision":6165,"compact-revision":5907}
{"level":"info","ts":"2025-07-25T15:11:25.248987Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6410}
{"level":"info","ts":"2025-07-25T15:11:25.258613Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":6410,"took":"8.264002ms","hash":402987699,"current-db-size-bytes":3158016,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1757184,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-07-25T15:11:25.258737Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":402987699,"revision":6410,"compact-revision":6165}
{"level":"info","ts":"2025-07-25T15:16:25.260525Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6650}
{"level":"info","ts":"2025-07-25T15:16:25.268247Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":6650,"took":"6.716084ms","hash":783603536,"current-db-size-bytes":3158016,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":2011136,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2025-07-25T15:16:25.268347Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":783603536,"revision":6650,"compact-revision":6410}
{"level":"info","ts":"2025-07-25T15:21:25.272366Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6949}
{"level":"info","ts":"2025-07-25T15:21:25.279750Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":6949,"took":"6.514451ms","hash":304876778,"current-db-size-bytes":3158016,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":2191360,"current-db-size-in-use":"2.2 MB"}
{"level":"info","ts":"2025-07-25T15:21:25.279857Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":304876778,"revision":6949,"compact-revision":6650}
{"level":"info","ts":"2025-07-25T15:26:25.282741Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7190}
{"level":"info","ts":"2025-07-25T15:26:25.290465Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":7190,"took":"6.72715ms","hash":2834721769,"current-db-size-bytes":3158016,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1736704,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-07-25T15:26:25.290598Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2834721769,"revision":7190,"compact-revision":6949}
{"level":"info","ts":"2025-07-25T15:31:25.295324Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7430}
{"level":"info","ts":"2025-07-25T15:31:25.303117Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":7430,"took":"6.735911ms","hash":1478459914,"current-db-size-bytes":3158016,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1691648,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-07-25T15:31:25.303242Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1478459914,"revision":7430,"compact-revision":7190}
{"level":"info","ts":"2025-07-25T15:36:25.308686Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7668}
{"level":"info","ts":"2025-07-25T15:36:25.316304Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":7668,"took":"6.203978ms","hash":1365701731,"current-db-size-bytes":3158016,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1708032,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-07-25T15:36:25.316506Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1365701731,"revision":7668,"compact-revision":7430}
{"level":"info","ts":"2025-07-25T15:40:42.257368Z","caller":"etcdserver/server.go:1476","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":10001,"local-member-snapshot-index":0,"local-member-snapshot-count":10000}
{"level":"info","ts":"2025-07-25T15:40:42.269485Z","caller":"etcdserver/server.go:2539","msg":"saved snapshot","snapshot-index":10001}
{"level":"info","ts":"2025-07-25T15:40:42.269791Z","caller":"etcdserver/server.go:2569","msg":"compacted Raft logs","compact-index":5001}
{"level":"info","ts":"2025-07-25T15:41:25.321857Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7907}
{"level":"info","ts":"2025-07-25T15:41:25.328873Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":7907,"took":"5.230025ms","hash":3339718041,"current-db-size-bytes":3158016,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1724416,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-07-25T15:41:25.329000Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3339718041,"revision":7907,"compact-revision":7668}
{"level":"info","ts":"2025-07-25T15:46:25.334251Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8146}
{"level":"info","ts":"2025-07-25T15:46:25.346351Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":8146,"took":"11.120596ms","hash":2145593746,"current-db-size-bytes":3158016,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1761280,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-07-25T15:46:25.346584Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2145593746,"revision":8146,"compact-revision":7907}
{"level":"info","ts":"2025-07-25T16:21:52.111253Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8384}
{"level":"info","ts":"2025-07-25T16:21:52.117878Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":8384,"took":"5.69285ms","hash":3439831522,"current-db-size-bytes":3158016,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1761280,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-07-25T16:21:52.117993Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3439831522,"revision":8384,"compact-revision":8146}
{"level":"info","ts":"2025-07-25T16:26:52.120277Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8623}
{"level":"info","ts":"2025-07-25T16:26:52.124687Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":8623,"took":"3.735858ms","hash":136885826,"current-db-size-bytes":3158016,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1748992,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-07-25T16:26:52.124780Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":136885826,"revision":8623,"compact-revision":8384}
{"level":"info","ts":"2025-07-25T16:31:52.133188Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8865}
{"level":"info","ts":"2025-07-25T16:31:52.144132Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":8865,"took":"9.680195ms","hash":2691117934,"current-db-size-bytes":3158016,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1826816,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-07-25T16:31:52.144378Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2691117934,"revision":8865,"compact-revision":8623}
{"level":"info","ts":"2025-07-25T16:36:52.145127Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":9105}
{"level":"info","ts":"2025-07-25T16:36:52.151387Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":9105,"took":"5.398953ms","hash":2193079973,"current-db-size-bytes":3158016,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1671168,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-07-25T16:36:52.151499Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2193079973,"revision":9105,"compact-revision":8865}
{"level":"info","ts":"2025-07-25T16:41:52.156723Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":9346}
{"level":"info","ts":"2025-07-25T16:41:52.163124Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":9346,"took":"5.334337ms","hash":2371618241,"current-db-size-bytes":3158016,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1638400,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2025-07-25T16:41:52.163239Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2371618241,"revision":9346,"compact-revision":9105}
{"level":"info","ts":"2025-07-25T16:46:52.167422Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":9584}
{"level":"info","ts":"2025-07-25T16:46:52.174461Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":9584,"took":"5.836511ms","hash":3444041285,"current-db-size-bytes":3158016,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1609728,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2025-07-25T16:46:52.174586Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3444041285,"revision":9584,"compact-revision":9346}
{"level":"info","ts":"2025-07-25T17:32:12.650756Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2025-07-25T17:32:12.652433Z","caller":"embed/etcd.go:408","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"info","ts":"2025-07-25T17:32:19.656615Z","caller":"etcdserver/server.go:1546","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"warn","ts":"2025-07-25T17:32:19.657756Z","caller":"embed/serve.go:235","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-07-25T17:32:19.658856Z","caller":"embed/serve.go:237","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-07-25T17:32:19.660352Z","caller":"embed/serve.go:235","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2025-07-25T17:32:19.660501Z","caller":"embed/serve.go:237","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2025-07-25T17:32:19.681647Z","caller":"embed/etcd.go:613","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-07-25T17:32:19.682460Z","caller":"embed/etcd.go:618","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-07-25T17:32:19.682582Z","caller":"embed/etcd.go:410","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> etcd [df316af689c8] <==
{"level":"info","ts":"2025-07-26T07:37:17.956028Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3164635072,"revision":15754,"compact-revision":15505}
{"level":"info","ts":"2025-07-26T07:39:44.887278Z","caller":"etcdserver/server.go:1476","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":20002,"local-member-snapshot-index":10001,"local-member-snapshot-count":10000}
{"level":"info","ts":"2025-07-26T07:39:44.906294Z","caller":"etcdserver/server.go:2539","msg":"saved snapshot","snapshot-index":20002}
{"level":"info","ts":"2025-07-26T07:39:44.906715Z","caller":"etcdserver/server.go:2569","msg":"compacted Raft logs","compact-index":15002}
{"level":"info","ts":"2025-07-26T07:42:17.961537Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15994}
{"level":"info","ts":"2025-07-26T07:42:17.968878Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":15994,"took":"6.495615ms","hash":2360498100,"current-db-size-bytes":3272704,"current-db-size":"3.3 MB","current-db-size-in-use-bytes":1740800,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-07-26T07:42:17.969043Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2360498100,"revision":15994,"compact-revision":15754}
{"level":"info","ts":"2025-07-26T07:47:17.976104Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16265}
{"level":"info","ts":"2025-07-26T07:47:17.986629Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":16265,"took":"9.924592ms","hash":3814579176,"current-db-size-bytes":3272704,"current-db-size":"3.3 MB","current-db-size-in-use-bytes":1744896,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-07-26T07:47:17.986755Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3814579176,"revision":16265,"compact-revision":15994}
{"level":"info","ts":"2025-07-26T07:52:17.990268Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16504}
{"level":"info","ts":"2025-07-26T07:52:17.996544Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":16504,"took":"5.737832ms","hash":467401415,"current-db-size-bytes":3272704,"current-db-size":"3.3 MB","current-db-size-in-use-bytes":1662976,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-07-26T07:52:17.996659Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":467401415,"revision":16504,"compact-revision":16265}
{"level":"info","ts":"2025-07-26T07:57:18.008933Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16742}
{"level":"info","ts":"2025-07-26T07:57:18.011580Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":16742,"took":"2.362549ms","hash":2930308725,"current-db-size-bytes":3567616,"current-db-size":"3.6 MB","current-db-size-in-use-bytes":1986560,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2025-07-26T07:57:18.011625Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2930308725,"revision":16742,"compact-revision":16504}
{"level":"info","ts":"2025-07-26T07:58:02.577303Z","caller":"traceutil/trace.go:171","msg":"trace[1597168581] transaction","detail":"{read_only:false; response_revision:17030; number_of_response:1; }","duration":"194.868073ms","start":"2025-07-26T07:58:02.382392Z","end":"2025-07-26T07:58:02.577260Z","steps":["trace[1597168581] 'process raft request'  (duration: 194.565019ms)"],"step_count":1}
{"level":"warn","ts":"2025-07-26T07:58:25.171346Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"159.79556ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-07-26T07:58:25.172182Z","caller":"traceutil/trace.go:171","msg":"trace[1790413603] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:17047; }","duration":"171.249668ms","start":"2025-07-26T07:58:25.000877Z","end":"2025-07-26T07:58:25.172127Z","steps":["trace[1790413603] 'range keys from in-memory index tree'  (duration: 159.609977ms)"],"step_count":1}
{"level":"info","ts":"2025-07-26T07:58:28.342287Z","caller":"traceutil/trace.go:171","msg":"trace[1654237609] transaction","detail":"{read_only:false; response_revision:17050; number_of_response:1; }","duration":"163.141979ms","start":"2025-07-26T07:58:28.179094Z","end":"2025-07-26T07:58:28.342236Z","steps":["trace[1654237609] 'process raft request'  (duration: 113.986669ms)","trace[1654237609] 'compare'  (duration: 48.968626ms)"],"step_count":2}
{"level":"info","ts":"2025-07-26T07:58:37.472081Z","caller":"traceutil/trace.go:171","msg":"trace[1643508259] transaction","detail":"{read_only:false; response_revision:17057; number_of_response:1; }","duration":"190.95112ms","start":"2025-07-26T07:58:37.281086Z","end":"2025-07-26T07:58:37.472037Z","steps":["trace[1643508259] 'process raft request'  (duration: 190.617903ms)"],"step_count":1}
{"level":"info","ts":"2025-07-26T07:58:49.777025Z","caller":"traceutil/trace.go:171","msg":"trace[633618837] transaction","detail":"{read_only:false; response_revision:17068; number_of_response:1; }","duration":"173.405249ms","start":"2025-07-26T07:58:49.603539Z","end":"2025-07-26T07:58:49.776944Z","steps":["trace[633618837] 'process raft request'  (duration: 173.061804ms)"],"step_count":1}
{"level":"info","ts":"2025-07-26T07:58:51.974583Z","caller":"traceutil/trace.go:171","msg":"trace[743563360] transaction","detail":"{read_only:false; response_revision:17069; number_of_response:1; }","duration":"174.583301ms","start":"2025-07-26T07:58:51.799950Z","end":"2025-07-26T07:58:51.974534Z","steps":["trace[743563360] 'process raft request'  (duration: 173.587999ms)"],"step_count":1}
{"level":"warn","ts":"2025-07-26T07:58:54.245508Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"152.869499ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-07-26T07:58:54.246428Z","caller":"traceutil/trace.go:171","msg":"trace[968825044] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:17070; }","duration":"153.781637ms","start":"2025-07-26T07:58:54.092563Z","end":"2025-07-26T07:58:54.246344Z","steps":["trace[968825044] 'range keys from in-memory index tree'  (duration: 152.768381ms)"],"step_count":1}
{"level":"info","ts":"2025-07-26T07:58:56.182976Z","caller":"traceutil/trace.go:171","msg":"trace[1242405783] transaction","detail":"{read_only:false; response_revision:17071; number_of_response:1; }","duration":"165.469485ms","start":"2025-07-26T07:58:56.017453Z","end":"2025-07-26T07:58:56.182922Z","steps":["trace[1242405783] 'process raft request'  (duration: 165.05658ms)"],"step_count":1}
{"level":"info","ts":"2025-07-26T07:58:58.443844Z","caller":"traceutil/trace.go:171","msg":"trace[1371889411] transaction","detail":"{read_only:false; response_revision:17074; number_of_response:1; }","duration":"221.254031ms","start":"2025-07-26T07:58:58.222530Z","end":"2025-07-26T07:58:58.443784Z","steps":["trace[1371889411] 'process raft request'  (duration: 220.589459ms)"],"step_count":1}
{"level":"info","ts":"2025-07-26T07:59:00.637182Z","caller":"traceutil/trace.go:171","msg":"trace[983931947] transaction","detail":"{read_only:false; response_revision:17076; number_of_response:1; }","duration":"171.220991ms","start":"2025-07-26T07:59:00.465905Z","end":"2025-07-26T07:59:00.637126Z","steps":["trace[983931947] 'process raft request'  (duration: 170.727815ms)"],"step_count":1}
{"level":"info","ts":"2025-07-26T07:59:02.830236Z","caller":"traceutil/trace.go:171","msg":"trace[456009986] transaction","detail":"{read_only:false; response_revision:17077; number_of_response:1; }","duration":"171.480703ms","start":"2025-07-26T07:59:02.658734Z","end":"2025-07-26T07:59:02.830215Z","steps":["trace[456009986] 'process raft request'  (duration: 171.311963ms)"],"step_count":1}
{"level":"info","ts":"2025-07-26T07:59:07.128314Z","caller":"traceutil/trace.go:171","msg":"trace[1344646739] transaction","detail":"{read_only:false; response_revision:17080; number_of_response:1; }","duration":"175.324918ms","start":"2025-07-26T07:59:06.952949Z","end":"2025-07-26T07:59:07.128274Z","steps":["trace[1344646739] 'process raft request'  (duration: 132.35907ms)","trace[1344646739] 'compare'  (duration: 42.260101ms)"],"step_count":2}
{"level":"info","ts":"2025-07-26T07:59:15.467264Z","caller":"traceutil/trace.go:171","msg":"trace[602018814] transaction","detail":"{read_only:false; response_revision:17086; number_of_response:1; }","duration":"170.359021ms","start":"2025-07-26T07:59:15.296865Z","end":"2025-07-26T07:59:15.467224Z","steps":["trace[602018814] 'process raft request'  (duration: 170.129997ms)"],"step_count":1}
{"level":"warn","ts":"2025-07-26T07:59:42.158693Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"153.166992ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-07-26T07:59:42.159062Z","caller":"traceutil/trace.go:171","msg":"trace[2021508808] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:17108; }","duration":"153.601737ms","start":"2025-07-26T07:59:42.005413Z","end":"2025-07-26T07:59:42.159015Z","steps":["trace[2021508808] 'range keys from in-memory index tree'  (duration: 153.013075ms)"],"step_count":1}
{"level":"warn","ts":"2025-07-26T07:59:44.237610Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"145.119795ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-07-26T07:59:44.237975Z","caller":"traceutil/trace.go:171","msg":"trace[1115473101] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:17109; }","duration":"145.5055ms","start":"2025-07-26T07:59:44.092400Z","end":"2025-07-26T07:59:44.237905Z","steps":["trace[1115473101] 'range keys from in-memory index tree'  (duration: 145.018648ms)"],"step_count":1}
{"level":"info","ts":"2025-07-26T07:59:46.194789Z","caller":"traceutil/trace.go:171","msg":"trace[2112512802] linearizableReadLoop","detail":"{readStateIndex:21247; appliedIndex:21246; }","duration":"101.719049ms","start":"2025-07-26T07:59:46.093015Z","end":"2025-07-26T07:59:46.194734Z","steps":["trace[2112512802] 'read index received'  (duration: 101.163233ms)","trace[2112512802] 'applied index is now lower than readState.Index'  (duration: 554.036µs)"],"step_count":2}
{"level":"warn","ts":"2025-07-26T07:59:46.195393Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"102.344752ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-07-26T07:59:46.197335Z","caller":"traceutil/trace.go:171","msg":"trace[1959687457] transaction","detail":"{read_only:false; response_revision:17110; number_of_response:1; }","duration":"205.350347ms","start":"2025-07-26T07:59:45.991907Z","end":"2025-07-26T07:59:46.197257Z","steps":["trace[1959687457] 'process raft request'  (duration: 202.490263ms)"],"step_count":1}
{"level":"info","ts":"2025-07-26T07:59:46.203151Z","caller":"traceutil/trace.go:171","msg":"trace[530515098] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:17110; }","duration":"102.513422ms","start":"2025-07-26T07:59:46.092994Z","end":"2025-07-26T07:59:46.195507Z","steps":["trace[530515098] 'agreement among raft nodes before linearized reading'  (duration: 102.300616ms)"],"step_count":1}
{"level":"info","ts":"2025-07-26T08:02:18.018582Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16995}
{"level":"info","ts":"2025-07-26T08:02:18.025038Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":16995,"took":"5.852486ms","hash":1452091078,"current-db-size-bytes":3567616,"current-db-size":"3.6 MB","current-db-size-in-use-bytes":2007040,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2025-07-26T08:02:18.025152Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1452091078,"revision":16995,"compact-revision":16742}
{"level":"info","ts":"2025-07-26T08:07:18.059503Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":17231}
{"level":"info","ts":"2025-07-26T08:07:18.066024Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":17231,"took":"5.92099ms","hash":2965411276,"current-db-size-bytes":3567616,"current-db-size":"3.6 MB","current-db-size-in-use-bytes":1744896,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-07-26T08:07:18.066134Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2965411276,"revision":17231,"compact-revision":16995}
{"level":"info","ts":"2025-07-26T08:12:18.072837Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":17471}
{"level":"info","ts":"2025-07-26T08:12:18.083169Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":17471,"took":"8.343998ms","hash":666294988,"current-db-size-bytes":3567616,"current-db-size":"3.6 MB","current-db-size-in-use-bytes":1736704,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-07-26T08:12:18.083311Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":666294988,"revision":17471,"compact-revision":17231}
{"level":"info","ts":"2025-07-26T08:17:18.084833Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":17710}
{"level":"info","ts":"2025-07-26T08:17:18.092832Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":17710,"took":"7.103518ms","hash":4288133644,"current-db-size-bytes":3567616,"current-db-size":"3.6 MB","current-db-size-in-use-bytes":1708032,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-07-26T08:17:18.092975Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":4288133644,"revision":17710,"compact-revision":17471}
{"level":"info","ts":"2025-07-26T08:25:21.677907Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":17948}
{"level":"info","ts":"2025-07-26T08:25:21.680717Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":17948,"took":"2.506526ms","hash":1888356200,"current-db-size-bytes":3567616,"current-db-size":"3.6 MB","current-db-size-in-use-bytes":1720320,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-07-26T08:25:21.680941Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1888356200,"revision":17948,"compact-revision":17710}
{"level":"info","ts":"2025-07-26T08:30:21.686572Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":18187}
{"level":"info","ts":"2025-07-26T08:30:21.693564Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":18187,"took":"5.893892ms","hash":1561325817,"current-db-size-bytes":3567616,"current-db-size":"3.6 MB","current-db-size-in-use-bytes":1765376,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-07-26T08:30:21.693643Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1561325817,"revision":18187,"compact-revision":17948}
{"level":"info","ts":"2025-07-26T08:35:21.696768Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":18426}
{"level":"info","ts":"2025-07-26T08:35:21.705213Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":18426,"took":"7.63966ms","hash":3752694333,"current-db-size-bytes":3665920,"current-db-size":"3.7 MB","current-db-size-in-use-bytes":1867776,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-07-26T08:35:21.705395Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3752694333,"revision":18426,"compact-revision":18187}


==> kernel <==
 08:39:51 up  3:33,  0 users,  load average: 1.24, 1.56, 1.74
Linux minikube 5.15.0-139-generic #149~20.04.1-Ubuntu SMP Wed Apr 16 08:29:56 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [145da57a89d1] <==
W0725 17:32:18.210096       1 logging.go:55] [core] [Channel #112 SubChannel #113]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:18.241452       1 logging.go:55] [core] [Channel #184 SubChannel #185]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:18.253821       1 logging.go:55] [core] [Channel #31 SubChannel #32]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:18.287246       1 logging.go:55] [core] [Channel #61 SubChannel #62]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:18.287247       1 logging.go:55] [core] [Channel #46 SubChannel #47]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:18.288545       1 logging.go:55] [core] [Channel #151 SubChannel #152]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:18.359841       1 logging.go:55] [core] [Channel #178 SubChannel #179]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:18.520377       1 logging.go:55] [core] [Channel #118 SubChannel #119]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:20.437187       1 logging.go:55] [core] [Channel #67 SubChannel #68]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:20.783116       1 logging.go:55] [core] [Channel #2 SubChannel #3]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:20.898145       1 logging.go:55] [core] [Channel #73 SubChannel #74]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:20.948031       1 logging.go:55] [core] [Channel #100 SubChannel #101]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:21.009447       1 logging.go:55] [core] [Channel #7 SubChannel #8]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:21.046692       1 logging.go:55] [core] [Channel #139 SubChannel #140]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:21.176309       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:21.187998       1 logging.go:55] [core] [Channel #181 SubChannel #182]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:21.406011       1 logging.go:55] [core] [Channel #52 SubChannel #53]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:21.406005       1 logging.go:55] [core] [Channel #37 SubChannel #38]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:21.535962       1 logging.go:55] [core] [Channel #76 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:21.573345       1 logging.go:55] [core] [Channel #133 SubChannel #134]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:21.611183       1 logging.go:55] [core] [Channel #172 SubChannel #173]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
I0725 17:32:21.642643       1 handler.go:288] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
W0725 17:32:21.655157       1 logging.go:55] [core] [Channel #34 SubChannel #35]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:21.676264       1 logging.go:55] [core] [Channel #178 SubChannel #179]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:21.684692       1 logging.go:55] [core] [Channel #127 SubChannel #128]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:21.717422       1 logging.go:55] [core] [Channel #184 SubChannel #185]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:21.724876       1 logging.go:55] [core] [Channel #10 SubChannel #615]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:21.736105       1 logging.go:55] [core] [Channel #112 SubChannel #113]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:21.779488       1 logging.go:55] [core] [Channel #148 SubChannel #149]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:21.824287       1 logging.go:55] [core] [Channel #103 SubChannel #104]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:21.828821       1 logging.go:55] [core] [Channel #64 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:21.829281       1 logging.go:55] [core] [Channel #166 SubChannel #167]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:21.829505       1 logging.go:55] [core] [Channel #130 SubChannel #131]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:21.861373       1 logging.go:55] [core] [Channel #31 SubChannel #32]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:21.866510       1 logging.go:55] [core] [Channel #85 SubChannel #86]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:21.892185       1 logging.go:55] [core] [Channel #17 SubChannel #18]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:21.898183       1 logging.go:55] [core] [Channel #154 SubChannel #155]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:21.902256       1 logging.go:55] [core] [Channel #88 SubChannel #89]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:21.950547       1 logging.go:55] [core] [Channel #49 SubChannel #50]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:21.950630       1 logging.go:55] [core] [Channel #61 SubChannel #62]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:21.950553       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:21.967430       1 logging.go:55] [core] [Channel #25 SubChannel #457]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:22.017393       1 logging.go:55] [core] [Channel #1 SubChannel #4]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:22.034427       1 logging.go:55] [core] [Channel #121 SubChannel #122]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:22.037653       1 logging.go:55] [core] [Channel #109 SubChannel #110]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:22.039116       1 logging.go:55] [core] [Channel #142 SubChannel #143]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:22.079249       1 logging.go:55] [core] [Channel #97 SubChannel #98]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:22.101337       1 logging.go:55] [core] [Channel #124 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:22.104702       1 logging.go:55] [core] [Channel #145 SubChannel #146]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:22.129624       1 logging.go:55] [core] [Channel #169 SubChannel #170]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:22.156863       1 logging.go:55] [core] [Channel #70 SubChannel #71]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:22.204526       1 logging.go:55] [core] [Channel #136 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:22.233104       1 logging.go:55] [core] [Channel #106 SubChannel #107]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:22.239698       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:22.269928       1 logging.go:55] [core] [Channel #115 SubChannel #116]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:22.282480       1 logging.go:55] [core] [Channel #160 SubChannel #161]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:22.296136       1 logging.go:55] [core] [Channel #28 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:22.416187       1 logging.go:55] [core] [Channel #79 SubChannel #80]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:22.416296       1 logging.go:55] [core] [Channel #58 SubChannel #59]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 17:32:22.562180       1 logging.go:55] [core] [Channel #163 SubChannel #164]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-apiserver [71c7de7e34ec] <==
I0726 05:19:25.430577       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0726 05:19:25.468283       1 controller.go:667] quota admission added evaluator for: daemonsets.apps
E0726 05:19:27.870988       1 remote_available_controller.go:448] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.106.193.93:443/apis/metrics.k8s.io/v1beta1: Get \"https://10.106.193.93:443/apis/metrics.k8s.io/v1beta1\": context deadline exceeded" logger="UnhandledError"
E0726 05:19:33.207819       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: error trying to reach service: dial tcp 10.106.193.93:443: connect: connection refused
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
E0726 05:19:33.209221       1 controller.go:102] "Unhandled Error" err=<
	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: error trying to reach service: dial tcp 10.106.193.93:443: connect: connection refused
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I0726 05:19:33.211256       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0726 05:19:34.227588       1 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
W0726 05:19:34.227854       1 handler_proxy.go:99] no RequestInfo found in the context
E0726 05:19:34.228060       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I0726 05:19:34.228687       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0726 05:19:35.229727       1 handler_proxy.go:99] no RequestInfo found in the context
E0726 05:19:35.230030       1 controller.go:102] "Unhandled Error" err=<
	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I0726 05:19:35.231116       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0726 05:19:35.232222       1 handler_proxy.go:99] no RequestInfo found in the context
E0726 05:19:35.232317       1 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
I0726 05:19:35.233604       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0726 05:19:59.104246       1 handler_proxy.go:99] no RequestInfo found in the context
E0726 05:19:59.104299       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
E0726 05:19:59.104303       1 remote_available_controller.go:448] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.106.193.93:443/apis/metrics.k8s.io/v1beta1: Get \"https://10.106.193.93:443/apis/metrics.k8s.io/v1beta1\": dial tcp 10.106.193.93:443: connect: connection refused" logger="UnhandledError"
E0726 05:19:59.108537       1 remote_available_controller.go:448] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.106.193.93:443/apis/metrics.k8s.io/v1beta1: Get \"https://10.106.193.93:443/apis/metrics.k8s.io/v1beta1\": dial tcp 10.106.193.93:443: connect: connection refused" logger="UnhandledError"
E0726 05:19:59.115301       1 remote_available_controller.go:448] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.106.193.93:443/apis/metrics.k8s.io/v1beta1: Get \"https://10.106.193.93:443/apis/metrics.k8s.io/v1beta1\": dial tcp 10.106.193.93:443: connect: connection refused" logger="UnhandledError"
I0726 05:19:59.186335       1 handler.go:288] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0726 05:47:20.965868       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0726 05:57:20.966388       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0726 06:07:20.966636       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0726 06:17:20.967303       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
E0726 06:20:29.656304       1 authentication.go:75] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0726 06:20:30.256878       1 authentication.go:75] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
I0726 06:27:20.967722       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0726 06:37:20.969181       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0726 06:47:20.969242       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0726 06:57:20.969402       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0726 07:07:20.970302       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0726 07:17:20.971078       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0726 07:27:20.971059       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0726 07:37:20.972040       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0726 07:47:20.973223       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0726 07:56:24.821087       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0726 07:57:16.044589       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0726 07:57:20.973115       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0726 08:07:20.974532       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0726 08:17:20.974965       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0726 08:30:24.562957       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0726 08:33:33.942139       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0726 08:33:33.941109       1 alloc.go:328] "allocated clusterIPs" service="default/nginx-service" clusterIPs={"IPv4":"10.100.47.152"}


==> kube-controller-manager [085901a5cfa0] <==
I0726 05:19:24.831416       1 controllermanager.go:741] "Warning: controller is disabled" controller="selinux-warning-controller"
I0726 05:19:24.835798       1 serviceaccounts_controller.go:114] "Starting service account controller" logger="serviceaccount-controller"
I0726 05:19:24.835823       1 shared_informer.go:350] "Waiting for caches to sync" controller="service account"
I0726 05:19:24.854267       1 shared_informer.go:350] "Waiting for caches to sync" controller="resource quota"
I0726 05:19:24.864801       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0726 05:19:24.879652       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I0726 05:19:24.879696       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I0726 05:19:24.927273       1 shared_informer.go:357] "Caches are synced" controller="node"
I0726 05:19:24.927416       1 shared_informer.go:357] "Caches are synced" controller="PV protection"
I0726 05:19:24.927431       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0726 05:19:24.927613       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0726 05:19:24.927637       1 shared_informer.go:350] "Waiting for caches to sync" controller="cidrallocator"
I0726 05:19:24.927652       1 shared_informer.go:357] "Caches are synced" controller="cidrallocator"
I0726 05:19:24.928715       1 shared_informer.go:357] "Caches are synced" controller="expand"
I0726 05:19:24.928837       1 shared_informer.go:357] "Caches are synced" controller="TTL"
I0726 05:19:24.928951       1 shared_informer.go:357] "Caches are synced" controller="namespace"
I0726 05:19:24.937049       1 shared_informer.go:357] "Caches are synced" controller="service account"
I0726 05:19:24.939309       1 shared_informer.go:357] "Caches are synced" controller="taint-eviction-controller"
I0726 05:19:24.939849       1 shared_informer.go:357] "Caches are synced" controller="stateful set"
I0726 05:19:24.939890       1 shared_informer.go:357] "Caches are synced" controller="ClusterRoleAggregator"
I0726 05:19:24.942442       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrapproving"
I0726 05:19:24.942499       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice_mirroring"
I0726 05:19:24.945419       1 shared_informer.go:357] "Caches are synced" controller="endpoint"
I0726 05:19:24.948578       1 shared_informer.go:357] "Caches are synced" controller="bootstrap_signer"
I0726 05:19:24.951200       1 shared_informer.go:357] "Caches are synced" controller="GC"
I0726 05:19:24.956071       1 shared_informer.go:357] "Caches are synced" controller="PVC protection"
I0726 05:19:24.971512       1 shared_informer.go:357] "Caches are synced" controller="disruption"
I0726 05:19:24.971677       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I0726 05:19:24.972047       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I0726 05:19:24.979244       1 shared_informer.go:357] "Caches are synced" controller="crt configmap"
I0726 05:19:25.027402       1 shared_informer.go:357] "Caches are synced" controller="taint"
I0726 05:19:25.027653       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0726 05:19:25.027790       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0726 05:19:25.028015       1 shared_informer.go:357] "Caches are synced" controller="ReplicaSet"
I0726 05:19:25.027860       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0726 05:19:25.028432       1 shared_informer.go:357] "Caches are synced" controller="persistent volume"
I0726 05:19:25.028667       1 shared_informer.go:357] "Caches are synced" controller="daemon sets"
I0726 05:19:25.029495       1 shared_informer.go:357] "Caches are synced" controller="service-cidr-controller"
I0726 05:19:25.032681       1 shared_informer.go:357] "Caches are synced" controller="ReplicationController"
I0726 05:19:25.032703       1 shared_informer.go:357] "Caches are synced" controller="ephemeral"
I0726 05:19:25.032727       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice"
I0726 05:19:25.032737       1 shared_informer.go:357] "Caches are synced" controller="HPA"
I0726 05:19:25.032796       1 shared_informer.go:357] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0726 05:19:25.032994       1 shared_informer.go:357] "Caches are synced" controller="deployment"
I0726 05:19:25.038434       1 shared_informer.go:350] "Waiting for caches to sync" controller="garbage collector"
I0726 05:19:25.059050       1 shared_informer.go:357] "Caches are synced" controller="job"
I0726 05:19:25.069038       1 shared_informer.go:357] "Caches are synced" controller="TTL after finished"
I0726 05:19:25.137019       1 shared_informer.go:357] "Caches are synced" controller="cronjob"
I0726 05:19:25.158010       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0726 05:19:25.230102       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0726 05:19:25.230236       1 shared_informer.go:357] "Caches are synced" controller="attach detach"
I0726 05:19:25.327252       1 shared_informer.go:357] "Caches are synced" controller="validatingadmissionpolicy-status"
I0726 05:19:25.727378       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0726 05:19:25.727450       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0726 05:19:25.727480       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0726 05:19:25.740062       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
E0726 05:19:55.171466       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0726 05:19:55.768058       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
E0726 06:20:29.665477       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I0726 06:20:30.263610       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"


==> kube-controller-manager [e581889c7bd4] <==
I0725 14:51:31.739683       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I0725 14:51:31.739862       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I0725 14:51:31.742470       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0725 14:51:31.743200       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrapproving"
I0725 14:51:31.804445       1 shared_informer.go:357] "Caches are synced" controller="TTL"
I0725 14:51:31.829025       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I0725 14:51:31.833808       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I0725 14:51:31.844088       1 shared_informer.go:357] "Caches are synced" controller="node"
I0725 14:51:31.845445       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0725 14:51:31.846594       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0725 14:51:31.846659       1 shared_informer.go:350] "Waiting for caches to sync" controller="cidrallocator"
I0725 14:51:31.846719       1 shared_informer.go:357] "Caches are synced" controller="cidrallocator"
I0725 14:51:31.918045       1 shared_informer.go:357] "Caches are synced" controller="taint"
I0725 14:51:31.918563       1 shared_informer.go:357] "Caches are synced" controller="HPA"
I0725 14:51:31.921005       1 shared_informer.go:357] "Caches are synced" controller="taint-eviction-controller"
I0725 14:51:31.921662       1 shared_informer.go:357] "Caches are synced" controller="PV protection"
I0725 14:51:31.930307       1 shared_informer.go:357] "Caches are synced" controller="disruption"
I0725 14:51:31.931378       1 shared_informer.go:357] "Caches are synced" controller="endpoint"
I0725 14:51:31.949396       1 shared_informer.go:357] "Caches are synced" controller="deployment"
I0725 14:51:32.012442       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0725 14:51:32.017204       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0725 14:51:32.018336       1 shared_informer.go:357] "Caches are synced" controller="GC"
I0725 14:51:32.022240       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0725 14:51:32.027795       1 shared_informer.go:357] "Caches are synced" controller="ReplicationController"
I0725 14:51:32.032507       1 shared_informer.go:357] "Caches are synced" controller="ReplicaSet"
I0725 14:51:32.014912       1 shared_informer.go:357] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0725 14:51:32.015897       1 shared_informer.go:357] "Caches are synced" controller="ephemeral"
I0725 14:51:32.109328       1 shared_informer.go:357] "Caches are synced" controller="job"
I0725 14:51:32.122363       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice_mirroring"
I0725 14:51:32.016294       1 shared_informer.go:357] "Caches are synced" controller="persistent volume"
I0725 14:51:32.129491       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0725 14:51:32.130591       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0725 14:51:32.203227       1 shared_informer.go:357] "Caches are synced" controller="attach detach"
I0725 14:51:32.204837       1 shared_informer.go:357] "Caches are synced" controller="stateful set"
I0725 14:51:32.204947       1 shared_informer.go:357] "Caches are synced" controller="PVC protection"
I0725 14:51:32.205048       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice"
I0725 14:51:32.208689       1 shared_informer.go:350] "Waiting for caches to sync" controller="garbage collector"
I0725 14:51:32.234892       1 shared_informer.go:357] "Caches are synced" controller="daemon sets"
I0725 14:51:32.239618       1 shared_informer.go:357] "Caches are synced" controller="namespace"
I0725 14:51:32.239751       1 shared_informer.go:357] "Caches are synced" controller="service account"
I0725 14:51:32.303951       1 shared_informer.go:357] "Caches are synced" controller="ClusterRoleAggregator"
I0725 14:51:32.633486       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0725 14:51:32.633606       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0725 14:51:32.633654       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0725 14:51:32.635670       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
E0725 14:51:51.212361       1 replica_set.go:562] "Unhandled Error" err="sync \"kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b\" failed with pods \"dashboard-metrics-scraper-5d59dccf9b-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
E0725 14:51:51.216320       1 replica_set.go:562] "Unhandled Error" err="sync \"kubernetes-dashboard/kubernetes-dashboard-7779f9b69b\" failed with pods \"kubernetes-dashboard-7779f9b69b-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
E0725 14:51:51.221217       1 replica_set.go:562] "Unhandled Error" err="sync \"kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b\" failed with pods \"dashboard-metrics-scraper-5d59dccf9b-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
E0725 14:51:51.223950       1 replica_set.go:562] "Unhandled Error" err="sync \"kubernetes-dashboard/kubernetes-dashboard-7779f9b69b\" failed with pods \"kubernetes-dashboard-7779f9b69b-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
E0725 14:51:51.227119       1 replica_set.go:562] "Unhandled Error" err="sync \"kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b\" failed with pods \"dashboard-metrics-scraper-5d59dccf9b-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
E0725 14:51:51.233553       1 replica_set.go:562] "Unhandled Error" err="sync \"kubernetes-dashboard/kubernetes-dashboard-7779f9b69b\" failed with pods \"kubernetes-dashboard-7779f9b69b-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
E0725 14:51:51.237777       1 replica_set.go:562] "Unhandled Error" err="sync \"kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b\" failed with pods \"dashboard-metrics-scraper-5d59dccf9b-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
E0725 15:12:02.820409       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0725 15:12:03.306633       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
E0725 15:12:32.845015       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0725 15:12:33.336218       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
E0725 15:13:02.863308       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0725 15:13:03.377233       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
E0725 16:41:01.879880       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I0725 16:41:02.213182       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"


==> kube-proxy [7035a1e0260c] <==
I0725 14:51:34.989257       1 server_linux.go:63] "Using iptables proxy"
I0725 14:51:35.294709       1 server.go:715] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0725 14:51:35.295706       1 server.go:245] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0725 14:51:35.381357       1 server.go:254] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0725 14:51:35.381408       1 server_linux.go:145] "Using iptables Proxier"
I0725 14:51:35.390821       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0725 14:51:35.411052       1 server.go:516] "Version info" version="v1.33.1"
I0725 14:51:35.411093       1 server.go:518] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0725 14:51:35.434010       1 config.go:199] "Starting service config controller"
I0725 14:51:35.434153       1 config.go:105] "Starting endpoint slice config controller"
I0725 14:51:35.438592       1 config.go:329] "Starting node config controller"
I0725 14:51:35.438734       1 shared_informer.go:350] "Waiting for caches to sync" controller="service config"
I0725 14:51:35.439572       1 shared_informer.go:350] "Waiting for caches to sync" controller="endpoint slice config"
I0725 14:51:35.447728       1 config.go:440] "Starting serviceCIDR config controller"
I0725 14:51:35.447744       1 shared_informer.go:350] "Waiting for caches to sync" controller="serviceCIDR config"
I0725 14:51:35.453769       1 shared_informer.go:350] "Waiting for caches to sync" controller="node config"
I0725 14:51:35.540421       1 shared_informer.go:357] "Caches are synced" controller="endpoint slice config"
I0725 14:51:35.548898       1 shared_informer.go:357] "Caches are synced" controller="serviceCIDR config"
I0725 14:51:35.551298       1 shared_informer.go:357] "Caches are synced" controller="service config"
I0725 14:51:35.567155       1 shared_informer.go:357] "Caches are synced" controller="node config"


==> kube-proxy [eba12fda0211] <==
I0726 05:19:26.871556       1 server_linux.go:63] "Using iptables proxy"
I0726 05:19:27.333355       1 server.go:715] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0726 05:19:27.333963       1 server.go:245] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0726 05:19:27.391660       1 server.go:254] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0726 05:19:27.391708       1 server_linux.go:145] "Using iptables Proxier"
I0726 05:19:27.399295       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0726 05:19:27.413996       1 server.go:516] "Version info" version="v1.33.1"
I0726 05:19:27.427234       1 server.go:518] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0726 05:19:27.437498       1 config.go:199] "Starting service config controller"
I0726 05:19:27.438978       1 config.go:105] "Starting endpoint slice config controller"
I0726 05:19:27.441195       1 shared_informer.go:350] "Waiting for caches to sync" controller="endpoint slice config"
I0726 05:19:27.441403       1 config.go:440] "Starting serviceCIDR config controller"
I0726 05:19:27.441533       1 shared_informer.go:350] "Waiting for caches to sync" controller="service config"
I0726 05:19:27.441535       1 shared_informer.go:350] "Waiting for caches to sync" controller="serviceCIDR config"
I0726 05:19:27.442064       1 config.go:329] "Starting node config controller"
I0726 05:19:27.442123       1 shared_informer.go:350] "Waiting for caches to sync" controller="node config"
I0726 05:19:27.541460       1 shared_informer.go:357] "Caches are synced" controller="endpoint slice config"
I0726 05:19:27.543231       1 shared_informer.go:357] "Caches are synced" controller="node config"
I0726 05:19:27.543247       1 shared_informer.go:357] "Caches are synced" controller="service config"
I0726 05:19:27.543267       1 shared_informer.go:357] "Caches are synced" controller="serviceCIDR config"


==> kube-scheduler [0aec3ec189bf] <==
I0726 05:19:16.418714       1 serving.go:386] Generated self-signed cert in-memory
I0726 05:19:21.483159       1 server.go:171] "Starting Kubernetes Scheduler" version="v1.33.1"
I0726 05:19:21.483299       1 server.go:173] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0726 05:19:21.529178       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I0726 05:19:21.533967       1 requestheader_controller.go:180] Starting RequestHeaderAuthRequestController
I0726 05:19:21.534012       1 shared_informer.go:350] "Waiting for caches to sync" controller="RequestHeaderAuthRequestController"
I0726 05:19:21.534071       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0726 05:19:21.536115       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0726 05:19:21.536133       1 shared_informer.go:350] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0726 05:19:21.536155       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0726 05:19:21.536168       1 shared_informer.go:350] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0726 05:19:21.634109       1 shared_informer.go:357] "Caches are synced" controller="RequestHeaderAuthRequestController"
I0726 05:19:21.636813       1 shared_informer.go:357] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0726 05:19:21.636906       1 shared_informer.go:357] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kube-scheduler [95cfd2e0878c] <==
I0725 14:51:19.742676       1 serving.go:386] Generated self-signed cert in-memory
W0725 14:51:26.503537       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0725 14:51:26.503609       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0725 14:51:26.503630       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0725 14:51:26.503644       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0725 14:51:26.654273       1 server.go:171] "Starting Kubernetes Scheduler" version="v1.33.1"
I0725 14:51:26.654344       1 server.go:173] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0725 14:51:26.665140       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I0725 14:51:26.698214       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0725 14:51:26.698298       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0725 14:51:26.705923       1 shared_informer.go:350] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0725 14:51:26.814673       1 shared_informer.go:357] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0725 17:32:12.754232       1 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
I0725 17:32:12.755876       1 configmap_cafile_content.go:226] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0725 17:32:12.756094       1 secure_serving.go:259] Stopped listening on 127.0.0.1:10259
E0725 17:32:12.756960       1 run.go:72] "command failed" err="finished without leader elect"


==> kubelet <==
Jul 26 05:19:17 minikube kubelet[1691]: E0726 05:19:17.951155    1691 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"kube-controller-manager-minikube\" already exists" pod="kube-system/kube-controller-manager-minikube"
Jul 26 05:19:17 minikube kubelet[1691]: I0726 05:19:17.951761    1691 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/kube-scheduler-minikube"
Jul 26 05:19:17 minikube kubelet[1691]: I0726 05:19:17.954293    1691 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Jul 26 05:19:17 minikube kubelet[1691]: I0726 05:19:17.983569    1691 desired_state_of_world_populator.go:158] "Finished populating initial desired state of world"
Jul 26 05:19:18 minikube kubelet[1691]: E0726 05:19:18.004800    1691 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Jul 26 05:19:18 minikube kubelet[1691]: I0726 05:19:18.004837    1691 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Jul 26 05:19:18 minikube kubelet[1691]: I0726 05:19:18.047593    1691 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/2cb21360-70bb-4ee8-a0ec-ec86d8b24a1e-lib-modules\") pod \"kube-proxy-hzrth\" (UID: \"2cb21360-70bb-4ee8-a0ec-ec86d8b24a1e\") " pod="kube-system/kube-proxy-hzrth"
Jul 26 05:19:18 minikube kubelet[1691]: I0726 05:19:18.048814    1691 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/2cb21360-70bb-4ee8-a0ec-ec86d8b24a1e-xtables-lock\") pod \"kube-proxy-hzrth\" (UID: \"2cb21360-70bb-4ee8-a0ec-ec86d8b24a1e\") " pod="kube-system/kube-proxy-hzrth"
Jul 26 05:19:18 minikube kubelet[1691]: E0726 05:19:18.055714    1691 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Jul 26 05:19:18 minikube kubelet[1691]: I0726 05:19:18.055741    1691 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Jul 26 05:19:18 minikube kubelet[1691]: E0726 05:19:18.150041    1691 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Jul 26 05:19:19 minikube kubelet[1691]: E0726 05:19:19.083733    1691 configmap.go:193] Couldn't get configMap kube-system/coredns: failed to sync configmap cache: timed out waiting for the condition
Jul 26 05:19:19 minikube kubelet[1691]: E0726 05:19:19.083803    1691 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/configmap/f960a04b-38b1-4924-b446-575c359df649-config-volume podName:f960a04b-38b1-4924-b446-575c359df649 nodeName:}" failed. No retries permitted until 2025-07-26 05:19:19.583783247 +0000 UTC m=+11.426926576 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "config-volume" (UniqueName: "kubernetes.io/configmap/f960a04b-38b1-4924-b446-575c359df649-config-volume") pod "coredns-674b8bbfcf-flhf9" (UID: "f960a04b-38b1-4924-b446-575c359df649") : failed to sync configmap cache: timed out waiting for the condition
Jul 26 05:19:20 minikube kubelet[1691]: E0726 05:19:20.042624    1691 summary_sys_containers.go:51] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Jul 26 05:19:20 minikube kubelet[1691]: E0726 05:19:20.042708    1691 helpers.go:851] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Jul 26 05:19:21 minikube kubelet[1691]: I0726 05:19:21.044718    1691 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="a8b93f3cc78e1ede5907b84aa63ccc1292f5ef5f4e620907f1f931b5398e2652"
Jul 26 05:19:22 minikube kubelet[1691]: I0726 05:19:22.372187    1691 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="1e33a8e265998a3fb06041c9a37b4f466be3439da6355dad50dcb8075df4a51e"
Jul 26 05:19:23 minikube kubelet[1691]: I0726 05:19:23.061406    1691 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="ee4a937cb8a0b04d7def2b6e5dc711d4e67f6d7febda76952c360fc81296b9c0"
Jul 26 05:19:23 minikube kubelet[1691]: I0726 05:19:23.205771    1691 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="1076d9a8cec67f2d180f8522d45612750270013ded6561614567dde80178414a"
Jul 26 05:19:24 minikube kubelet[1691]: I0726 05:19:24.236682    1691 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="046308db68868b062ab180b27baa469fd9009cac252a67b74688cafe9b9e4675"
Jul 26 05:19:30 minikube kubelet[1691]: E0726 05:19:30.115263    1691 summary_sys_containers.go:51] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Jul 26 05:19:30 minikube kubelet[1691]: E0726 05:19:30.115300    1691 helpers.go:851] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Jul 26 05:19:40 minikube kubelet[1691]: E0726 05:19:40.173965    1691 summary_sys_containers.go:51] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Jul 26 05:19:40 minikube kubelet[1691]: E0726 05:19:40.174014    1691 helpers.go:851] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Jul 26 05:19:50 minikube kubelet[1691]: E0726 05:19:50.234359    1691 summary_sys_containers.go:51] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Jul 26 05:19:50 minikube kubelet[1691]: E0726 05:19:50.234408    1691 helpers.go:851] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Jul 26 05:19:55 minikube kubelet[1691]: I0726 05:19:55.730514    1691 scope.go:117] "RemoveContainer" containerID="6b0b036ef5e6651e08906ec603eca56e7fd21f9a24ea52c530035bc03aa39347"
Jul 26 05:19:55 minikube kubelet[1691]: I0726 05:19:55.731368    1691 scope.go:117] "RemoveContainer" containerID="bfadcc763bf34beca69d36b66b5ab1826f3796b316a2b7f5469e3eaa8e7d6b27"
Jul 26 05:19:55 minikube kubelet[1691]: E0726 05:19:55.731687    1691 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(9fbdf6ee-403b-43f3-8ed7-9ccc9e6f2ec9)\"" pod="kube-system/storage-provisioner" podUID="9fbdf6ee-403b-43f3-8ed7-9ccc9e6f2ec9"
Jul 26 05:19:56 minikube kubelet[1691]: I0726 05:19:56.880057    1691 scope.go:117] "RemoveContainer" containerID="5056d9dc029b11c2245da3d12155e7c619dbbb0d28016aa82b617c0eaaa3e87a"
Jul 26 05:19:57 minikube kubelet[1691]: E0726 05:19:57.522724    1691 summary_sys_containers.go:89] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Jul 26 05:19:57 minikube kubelet[1691]: I0726 05:19:57.994252    1691 status_manager.go:355] "Container readiness changed for unknown container" pod="kube-system/metrics-server-7fbb699795-6dn7t" containerID="docker://2ca667839b52dc27854468effaeb49d111ef7e9ee485b70d4d38ac7d7ea83f1e"
Jul 26 05:19:58 minikube kubelet[1691]: I0726 05:19:58.009867    1691 scope.go:117] "RemoveContainer" containerID="310f7b77ac17a3f81fc8bb8a4612f06ef9bcaef1183ef9d093b3a418a062361c"
Jul 26 05:19:58 minikube kubelet[1691]: I0726 05:19:58.010446    1691 scope.go:117] "RemoveContainer" containerID="ef59affb95b21f9819a76058c9fe2cf8ac6f90e4b1b948e8e990b2252eddb714"
Jul 26 05:19:58 minikube kubelet[1691]: E0726 05:19:58.010678    1691 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with CrashLoopBackOff: \"back-off 10s restarting failed container=kubernetes-dashboard pod=kubernetes-dashboard-7779f9b69b-ggp6t_kubernetes-dashboard(801f6f23-0cc9-41b3-9599-45341d942a37)\"" pod="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b-ggp6t" podUID="801f6f23-0cc9-41b3-9599-45341d942a37"
Jul 26 05:20:00 minikube kubelet[1691]: E0726 05:20:00.287951    1691 summary_sys_containers.go:51] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Jul 26 05:20:00 minikube kubelet[1691]: E0726 05:20:00.288012    1691 helpers.go:851] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Jul 26 05:20:01 minikube kubelet[1691]: I0726 05:20:01.370272    1691 scope.go:117] "RemoveContainer" containerID="ef59affb95b21f9819a76058c9fe2cf8ac6f90e4b1b948e8e990b2252eddb714"
Jul 26 05:20:01 minikube kubelet[1691]: E0726 05:20:01.370554    1691 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with CrashLoopBackOff: \"back-off 10s restarting failed container=kubernetes-dashboard pod=kubernetes-dashboard-7779f9b69b-ggp6t_kubernetes-dashboard(801f6f23-0cc9-41b3-9599-45341d942a37)\"" pod="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b-ggp6t" podUID="801f6f23-0cc9-41b3-9599-45341d942a37"
Jul 26 05:20:10 minikube kubelet[1691]: I0726 05:20:10.729651    1691 scope.go:117] "RemoveContainer" containerID="bfadcc763bf34beca69d36b66b5ab1826f3796b316a2b7f5469e3eaa8e7d6b27"
Jul 26 05:20:13 minikube kubelet[1691]: I0726 05:20:13.729498    1691 scope.go:117] "RemoveContainer" containerID="ef59affb95b21f9819a76058c9fe2cf8ac6f90e4b1b948e8e990b2252eddb714"
Jul 26 06:02:26 minikube kubelet[1691]: I0726 06:02:26.657329    1691 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"kube-api-access-mhlbs\" (UniqueName: \"kubernetes.io/projected/9f2181c0-2e89-4f50-aad4-1f502d7b9e65-kube-api-access-mhlbs\") pod \"9f2181c0-2e89-4f50-aad4-1f502d7b9e65\" (UID: \"9f2181c0-2e89-4f50-aad4-1f502d7b9e65\") "
Jul 26 06:02:26 minikube kubelet[1691]: I0726 06:02:26.671692    1691 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/9f2181c0-2e89-4f50-aad4-1f502d7b9e65-kube-api-access-mhlbs" (OuterVolumeSpecName: "kube-api-access-mhlbs") pod "9f2181c0-2e89-4f50-aad4-1f502d7b9e65" (UID: "9f2181c0-2e89-4f50-aad4-1f502d7b9e65"). InnerVolumeSpecName "kube-api-access-mhlbs". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Jul 26 06:02:26 minikube kubelet[1691]: I0726 06:02:26.758813    1691 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-mhlbs\" (UniqueName: \"kubernetes.io/projected/9f2181c0-2e89-4f50-aad4-1f502d7b9e65-kube-api-access-mhlbs\") on node \"minikube\" DevicePath \"\""
Jul 26 06:02:27 minikube kubelet[1691]: I0726 06:02:27.099946    1691 scope.go:117] "RemoveContainer" containerID="c1e46574662d4d594214ee6691b1e832b3b438203e0a6fe3d3cd01163e4a0250"
Jul 26 06:02:27 minikube kubelet[1691]: I0726 06:02:27.123090    1691 scope.go:117] "RemoveContainer" containerID="ba2ba69e0da99dcf69c8e179ecc26db961fb33b34676c89848a5068b153e5b9d"
Jul 26 06:02:27 minikube kubelet[1691]: I0726 06:02:27.138354    1691 scope.go:117] "RemoveContainer" containerID="c1e46574662d4d594214ee6691b1e832b3b438203e0a6fe3d3cd01163e4a0250"
Jul 26 06:02:27 minikube kubelet[1691]: E0726 06:02:27.143516    1691 log.go:32] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: c1e46574662d4d594214ee6691b1e832b3b438203e0a6fe3d3cd01163e4a0250" containerID="c1e46574662d4d594214ee6691b1e832b3b438203e0a6fe3d3cd01163e4a0250"
Jul 26 06:02:27 minikube kubelet[1691]: I0726 06:02:27.143959    1691 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"c1e46574662d4d594214ee6691b1e832b3b438203e0a6fe3d3cd01163e4a0250"} err="failed to get container status \"c1e46574662d4d594214ee6691b1e832b3b438203e0a6fe3d3cd01163e4a0250\": rpc error: code = Unknown desc = Error response from daemon: No such container: c1e46574662d4d594214ee6691b1e832b3b438203e0a6fe3d3cd01163e4a0250"
Jul 26 06:02:27 minikube kubelet[1691]: I0726 06:02:27.143990    1691 scope.go:117] "RemoveContainer" containerID="ba2ba69e0da99dcf69c8e179ecc26db961fb33b34676c89848a5068b153e5b9d"
Jul 26 06:02:27 minikube kubelet[1691]: E0726 06:02:27.144780    1691 log.go:32] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: ba2ba69e0da99dcf69c8e179ecc26db961fb33b34676c89848a5068b153e5b9d" containerID="ba2ba69e0da99dcf69c8e179ecc26db961fb33b34676c89848a5068b153e5b9d"
Jul 26 06:02:27 minikube kubelet[1691]: I0726 06:02:27.144819    1691 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"ba2ba69e0da99dcf69c8e179ecc26db961fb33b34676c89848a5068b153e5b9d"} err="failed to get container status \"ba2ba69e0da99dcf69c8e179ecc26db961fb33b34676c89848a5068b153e5b9d\": rpc error: code = Unknown desc = Error response from daemon: No such container: ba2ba69e0da99dcf69c8e179ecc26db961fb33b34676c89848a5068b153e5b9d"
Jul 26 06:02:27 minikube kubelet[1691]: I0726 06:02:27.971036    1691 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="9f2181c0-2e89-4f50-aad4-1f502d7b9e65" path="/var/lib/kubelet/pods/9f2181c0-2e89-4f50-aad4-1f502d7b9e65/volumes"
Jul 26 07:28:46 minikube kubelet[1691]: I0726 07:28:46.811481    1691 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-5mh9v\" (UniqueName: \"kubernetes.io/projected/5e2b24ac-8ba4-4eed-8811-41d678b0097c-kube-api-access-5mh9v\") pod \"nginx-pod\" (UID: \"5e2b24ac-8ba4-4eed-8811-41d678b0097c\") " pod="default/nginx-pod"
Jul 26 07:35:14 minikube kubelet[1691]: I0726 07:35:14.938269    1691 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/nginx-pod" podStartSLOduration=386.047635449 podStartE2EDuration="6m28.93781289s" podCreationTimestamp="2025-07-26 07:28:46 +0000 UTC" firstStartedPulling="2025-07-26 07:28:47.414461353 +0000 UTC m=+6696.030757431" lastFinishedPulling="2025-07-26 07:28:50.304638704 +0000 UTC m=+6698.920934872" observedRunningTime="2025-07-26 07:28:51.069787701 +0000 UTC m=+6699.686083830" watchObservedRunningTime="2025-07-26 07:35:14.93781289 +0000 UTC m=+7083.554108993"
Jul 26 07:41:51 minikube kubelet[1691]: I0726 07:41:51.433894    1691 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-7f5rc\" (UniqueName: \"kubernetes.io/projected/cbe1d2a0-a38b-4012-8251-312526819b59-kube-api-access-7f5rc\") pod \"nginx-deployment-6858ccbc5-r679z\" (UID: \"cbe1d2a0-a38b-4012-8251-312526819b59\") " pod="default/nginx-deployment-6858ccbc5-r679z"
Jul 26 07:41:51 minikube kubelet[1691]: I0726 07:41:51.434284    1691 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-h488r\" (UniqueName: \"kubernetes.io/projected/36cac946-b9b6-4cd8-a325-c4f997e9265d-kube-api-access-h488r\") pod \"nginx-deployment-6858ccbc5-4x7d5\" (UID: \"36cac946-b9b6-4cd8-a325-c4f997e9265d\") " pod="default/nginx-deployment-6858ccbc5-4x7d5"
Jul 26 07:41:51 minikube kubelet[1691]: I0726 07:41:51.897519    1691 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="4285b55eda80ddb80b9d0e03d7738fb21bcffaa3289e7794ab45465d7a6f23af"
Jul 26 07:41:51 minikube kubelet[1691]: I0726 07:41:51.902063    1691 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="73fd17532554391f1fe8bd3bca4cf21e5e641d4d2d3e567b81531a2775a1d860"
Jul 26 07:41:56 minikube kubelet[1691]: I0726 07:41:56.198608    1691 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/nginx-deployment-6858ccbc5-r679z" podStartSLOduration=2.243696016 podStartE2EDuration="5.198576604s" podCreationTimestamp="2025-07-26 07:41:51 +0000 UTC" firstStartedPulling="2025-07-26 07:41:51.975875892 +0000 UTC m=+7480.592171985" lastFinishedPulling="2025-07-26 07:41:54.930756491 +0000 UTC m=+7483.547052573" observedRunningTime="2025-07-26 07:41:56.197691098 +0000 UTC m=+7484.813987201" watchObservedRunningTime="2025-07-26 07:41:56.198576604 +0000 UTC m=+7484.814872698"


==> kubernetes-dashboard [ef59affb95b2] <==
2025/07/26 05:19:27 Using namespace: kubernetes-dashboard
2025/07/26 05:19:27 Using in-cluster config to connect to apiserver
2025/07/26 05:19:27 Using secret token for csrf signing
2025/07/26 05:19:27 Initializing csrf token from kubernetes-dashboard-csrf secret
2025/07/26 05:19:27 Starting overwatch
panic: Get "https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/secrets/kubernetes-dashboard-csrf": dial tcp 10.96.0.1:443: i/o timeout

goroutine 1 [running]:
github.com/kubernetes/dashboard/src/app/backend/client/csrf.(*csrfTokenManager).init(0xc0000f9ae8)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:41 +0x30e
github.com/kubernetes/dashboard/src/app/backend/client/csrf.NewCsrfTokenManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:66
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).initCSRFKey(0xc000486680)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:527 +0x94
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).init(0x19aba3a?)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:495 +0x32
github.com/kubernetes/dashboard/src/app/backend/client.NewClientManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:594
main.main()
	/home/runner/work/dashboard/dashboard/src/app/backend/dashboard.go:96 +0x1cf


==> kubernetes-dashboard [f88ddf17cb73] <==
2025/07/26 05:20:13 Using namespace: kubernetes-dashboard
2025/07/26 05:20:13 Using in-cluster config to connect to apiserver
2025/07/26 05:20:13 Using secret token for csrf signing
2025/07/26 05:20:13 Initializing csrf token from kubernetes-dashboard-csrf secret
2025/07/26 05:20:13 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2025/07/26 05:20:13 Successful initial request to the apiserver, version: v1.33.1
2025/07/26 05:20:13 Generating JWE encryption key
2025/07/26 05:20:13 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2025/07/26 05:20:13 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2025/07/26 05:20:14 Initializing JWE encryption key from synchronized object
2025/07/26 05:20:14 Creating in-cluster Sidecar client
2025/07/26 05:20:14 Successful request to sidecar
2025/07/26 05:20:14 Serving insecurely on HTTP port: 9090
2025/07/26 05:20:13 Starting overwatch


==> storage-provisioner [0042ffbaf830] <==
W0726 08:38:52.503799       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:38:52.518612       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:38:54.524806       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:38:54.537439       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:38:56.546045       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:38:56.559683       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:38:58.566151       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:38:58.577144       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:00.587034       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:00.600986       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:02.611088       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:02.621798       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:04.628407       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:04.640274       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:06.649908       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:06.678563       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:08.688182       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:08.704608       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:10.711390       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:10.722493       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:12.730739       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:12.745772       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:14.754792       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:14.771334       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:16.780980       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:16.796040       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:18.803783       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:18.816741       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:20.822804       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:20.829670       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:22.839165       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:22.853887       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:24.863259       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:24.881793       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:26.890918       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:26.904346       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:28.912292       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:28.929096       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:30.932761       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:30.938785       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:32.941801       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:32.946174       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:34.954452       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:34.968154       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:36.973060       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:36.980314       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:38.985634       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:38.997022       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:41.003514       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:41.012741       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:43.019350       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:43.030788       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:45.037666       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:45.049333       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:47.057547       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:47.066967       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:49.075883       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:49.099425       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:51.103046       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0726 08:39:51.108273       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice


==> storage-provisioner [bfadcc763bf3] <==
I0726 05:19:25.165357       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0726 05:19:55.181774       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

